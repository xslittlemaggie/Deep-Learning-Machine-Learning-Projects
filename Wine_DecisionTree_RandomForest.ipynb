{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Decision Tree from scratch to complete & Limitations.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xslittlemaggie/Deep-Learning-Machine-Learning-Projects/blob/master/Wine_DecisionTree_RandomForest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8Mw5oMHk3xd",
        "colab_type": "text"
      },
      "source": [
        "<h1><center> Wine with Decision Tree & Random Forest  </center></h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_Ei21woLs3U",
        "colab_type": "text"
      },
      "source": [
        "<h1>Table of contents</h1>\n",
        "\n",
        "<div class=\"alert alert-block alert-info\" style=\"margin-top: 24px\">\n",
        "    <ol>\n",
        "        <li><a href=\"#Part One\">Part One: Decision Tree from scratch</a></li>\n",
        "        <li><a href=\"#Part Two\">Part Two: Decision Tree from sklearn</a></li>\n",
        "      <li><a href=\"#Part Three\">Part Three: Random Forest from sklearn</a></li>\n",
        "    </ol>\n",
        "</div>\n",
        "<br>\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRGZZBUX7PnQ",
        "colab_type": "text"
      },
      "source": [
        "# Part One: Realization of Decesion Tree from Scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBiW1j5lv7G5",
        "colab_type": "text"
      },
      "source": [
        "## Step 1: Calculate gini impurity\n",
        "The lower the better, more pure"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PD8HvuPmv13n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "def gini(labels):\n",
        "    impurity = 1\n",
        "    label_counts = Counter(labels)\n",
        "    for label in label_counts:\n",
        "      probability_of_label = label_counts[label]/len(labels)\n",
        "      impurity -= probability_of_label ** 2\n",
        "    return impurity"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhZKKvCDwP6z",
        "colab_type": "text"
      },
      "source": [
        "## Step 2: Calculate information gain"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDxnjKZ6t35f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def information_gain(starting_labels, split_labels):\n",
        "  info_gain = gini(starting_labels)\n",
        "  for subset in split_labels:\n",
        "    info_gain -= gini(subset) * len(subset)/len(starting_labels)\n",
        "  return info_gain"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apkI8QfLtrmG",
        "colab_type": "text"
      },
      "source": [
        "## Step 3: Split data based on different features (column)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJ2AxXSm3SQi",
        "colab_type": "text"
      },
      "source": [
        "#### 3.1 split data based on each feature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQ6_GPJuqyS5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cars = [['med', 'low', '3', '4', 'med', 'med'], \n",
        "        ['med', 'vhigh', '4', 'more', 'small', 'high'], \n",
        "        ['high', 'med', '3', '2', 'med', 'low'], \n",
        "        ['med', 'low', '4', '4', 'med', 'low'], \n",
        "        ['med', 'low', '5more', '2', 'big', 'med'], \n",
        "        ['med', 'med', '2', 'more', 'big', 'high'], \n",
        "        ['med', 'med', '2', 'more', 'med', 'med'], \n",
        "        ['vhigh', 'vhigh', '2', '2', 'med', 'low'], \n",
        "        ['high', 'med', '4', '2', 'big', 'low'], \n",
        "        ['low', 'low', '2', '4', 'big', 'med']]\n",
        "\n",
        "\n",
        "car_labels = ['acc', 'acc', 'unacc', 'unacc', 'unacc', 'vgood', 'acc', 'unacc', 'unacc', 'good']\n",
        "\n",
        "def split(dataset, labels, column):\n",
        "    data_subsets = []\n",
        "    label_subsets = []\n",
        "    counts = list(set([data[column] for data in dataset]))\n",
        "    counts.sort()\n",
        "    for k in counts:\n",
        "        new_data_subset = []\n",
        "        new_label_subset = []\n",
        "        for i in range(len(dataset)):\n",
        "            if dataset[i][column] == k:\n",
        "                new_data_subset.append(dataset[i])\n",
        "                new_label_subset.append(labels[i])\n",
        "        data_subsets.append(new_data_subset)\n",
        "        label_subsets.append(new_label_subset)\n",
        "    return information_gain(labels, label_subsets)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvs_cVbkuxth",
        "colab_type": "code",
        "outputId": "5391bb34-9efc-40a0-bc65-0f0d48d59d15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "for i in range(6):\n",
        "  data_subsets, label_subsets = split(cars, car_labels, i)\n",
        "  print(\"The information gain for feature {} is: {:.4f}.\".format(i + 1, information_gain(car_labels, label_subsets)))\n",
        " "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The information gain for feature 1 is: 0.2733.\n",
            "The information gain for feature 2 is: 0.0400.\n",
            "The information gain for feature 3 is: 0.1067.\n",
            "The information gain for feature 4 is: 0.3067.\n",
            "The information gain for feature 5 is: 0.1500.\n",
            "The information gain for feature 6 is: 0.2900.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtywFsz23Rzy",
        "colab_type": "text"
      },
      "source": [
        "#### 3.2 split data based on best feature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qd7M1OPW3Z7F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_best_split(dataset, labels):\n",
        "    best_gain = 0\n",
        "    best_feature = 0\n",
        "    for feature in range(len(dataset[0])):\n",
        "        data_subsets, label_subsets = split(dataset, labels, feature)\n",
        "        gain = information_gain(labels, label_subsets)\n",
        "        if gain > best_gain:\n",
        "            best_gain, best_feature = gain, feature\n",
        "    return best_feature, best_gain"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpbn51UL3-QR",
        "colab_type": "code",
        "outputId": "ff028d00-ad7b-4508-e85d-6fa8b0786fe2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "find_best_split(cars, car_labels)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, 0.3066666666666667)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_Jl9_QXzu6q",
        "colab_type": "text"
      },
      "source": [
        "Thus, from the result above, when split the data with the fourth feature (indexed from 0), the information gain is highest. This is the best feature to split on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztaFgSyZ0X8I",
        "colab_type": "text"
      },
      "source": [
        "## Step 4: Recursive Tree Building\n",
        "Now that we can find the best feture to split the dataset, we can repeate this process again and again to create the full tree. This is cursive algorithm! We start with every data point from the trinign set, find the best feature to split the data, split the data based on that feature, and then recursively repeat the process again on each subset that was created from the split.\n",
        "\n",
        "We'll stop the recursion when we can no longer find a feature that results in any information gain. In other words, we want to create a leaf of the tree when we can't find a way to split the data that makes purer subsets.\n",
        "\n",
        "The leaf should keep track of the classes of the data points from the trining set that ended up in the leaf. In our inplementation, we'll use a Counter object to keep track of the counts of labels. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-oQgjoGu9pe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_tree(data, labels):\n",
        "  best_feature, best_gain = find_best_split(data, labels)\n",
        "  if best_gain == 0:\n",
        "    return Counter(labels)\n",
        "  data_subsets, label_subsets = split(data, labels, best_feature)\n",
        "  branches = []\n",
        "  for i in range(len(data_subsets)):\n",
        "    tree = build_tree(data_subsets[i], label_subsets[i])\n",
        "    branches.append(tree)\n",
        "  return branches\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NwhCf_f7Z0y",
        "colab_type": "text"
      },
      "source": [
        "# Part Two: Decision Tree in scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFtu2i227dLi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0PJi64q8JL0",
        "colab_type": "text"
      },
      "source": [
        "### 1. upload the wine data for practice"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFfh1u127lpT",
        "colab_type": "code",
        "outputId": "114a0d4f-6bbb-4d69-80ca-5b857e64015b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        }
      },
      "source": [
        "df_wine = pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data', header=None)\n",
        "df_wine.columns = ['Class label', 'Alcohol', 'Malic acid', 'Ash', 'Alcalinity of ash', 'Magnesium', \n",
        "                   'Total phenols', 'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins', \n",
        "                   'Color intensity', 'Hue', 'OD280/OD315 of diluted wines', 'Proline']\n",
        "\n",
        "df_wine.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Class label</th>\n",
              "      <th>Alcohol</th>\n",
              "      <th>Malic acid</th>\n",
              "      <th>Ash</th>\n",
              "      <th>Alcalinity of ash</th>\n",
              "      <th>Magnesium</th>\n",
              "      <th>Total phenols</th>\n",
              "      <th>Flavanoids</th>\n",
              "      <th>Nonflavanoid phenols</th>\n",
              "      <th>Proanthocyanins</th>\n",
              "      <th>Color intensity</th>\n",
              "      <th>Hue</th>\n",
              "      <th>OD280/OD315 of diluted wines</th>\n",
              "      <th>Proline</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>14.23</td>\n",
              "      <td>1.71</td>\n",
              "      <td>2.43</td>\n",
              "      <td>15.6</td>\n",
              "      <td>127</td>\n",
              "      <td>2.80</td>\n",
              "      <td>3.06</td>\n",
              "      <td>0.28</td>\n",
              "      <td>2.29</td>\n",
              "      <td>5.64</td>\n",
              "      <td>1.04</td>\n",
              "      <td>3.92</td>\n",
              "      <td>1065</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>13.20</td>\n",
              "      <td>1.78</td>\n",
              "      <td>2.14</td>\n",
              "      <td>11.2</td>\n",
              "      <td>100</td>\n",
              "      <td>2.65</td>\n",
              "      <td>2.76</td>\n",
              "      <td>0.26</td>\n",
              "      <td>1.28</td>\n",
              "      <td>4.38</td>\n",
              "      <td>1.05</td>\n",
              "      <td>3.40</td>\n",
              "      <td>1050</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>13.16</td>\n",
              "      <td>2.36</td>\n",
              "      <td>2.67</td>\n",
              "      <td>18.6</td>\n",
              "      <td>101</td>\n",
              "      <td>2.80</td>\n",
              "      <td>3.24</td>\n",
              "      <td>0.30</td>\n",
              "      <td>2.81</td>\n",
              "      <td>5.68</td>\n",
              "      <td>1.03</td>\n",
              "      <td>3.17</td>\n",
              "      <td>1185</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>14.37</td>\n",
              "      <td>1.95</td>\n",
              "      <td>2.50</td>\n",
              "      <td>16.8</td>\n",
              "      <td>113</td>\n",
              "      <td>3.85</td>\n",
              "      <td>3.49</td>\n",
              "      <td>0.24</td>\n",
              "      <td>2.18</td>\n",
              "      <td>7.80</td>\n",
              "      <td>0.86</td>\n",
              "      <td>3.45</td>\n",
              "      <td>1480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>13.24</td>\n",
              "      <td>2.59</td>\n",
              "      <td>2.87</td>\n",
              "      <td>21.0</td>\n",
              "      <td>118</td>\n",
              "      <td>2.80</td>\n",
              "      <td>2.69</td>\n",
              "      <td>0.39</td>\n",
              "      <td>1.82</td>\n",
              "      <td>4.32</td>\n",
              "      <td>1.04</td>\n",
              "      <td>2.93</td>\n",
              "      <td>735</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Class label  Alcohol  ...  OD280/OD315 of diluted wines  Proline\n",
              "0            1    14.23  ...                          3.92     1065\n",
              "1            1    13.20  ...                          3.40     1050\n",
              "2            1    13.16  ...                          3.17     1185\n",
              "3            1    14.37  ...                          3.45     1480\n",
              "4            1    13.24  ...                          2.93      735\n",
              "\n",
              "[5 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjKUoKmy9VAY",
        "colab_type": "text"
      },
      "source": [
        "### 2. data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZF-wZVT9Xd0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = df_wine['Class label'].values  # all 3 classes\n",
        "X = df_wine.values[:, 1:]   # all features\n",
        "\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)  # transform label to 0 and 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1QfBvi79680",
        "colab_type": "code",
        "outputId": "10c7c033-9bfa-4dac-e7d8-86cd1dcc3aad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "X.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(178, 13)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_6F_PWj99qn",
        "colab_type": "code",
        "outputId": "5b978d63-2765-4799-af63-aa9f88e06e6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "y.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(178,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awlEdwFj9GCR",
        "colab_type": "text"
      },
      "source": [
        "### 3. split the data into training, validatioin datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GmWAQWrI9LE6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)  # choose 20% of the data as validation dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3xaYisj-U6D",
        "colab_type": "text"
      },
      "source": [
        "### 4. create and train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNx6ER888MZw",
        "colab_type": "code",
        "outputId": "ecf70459-e18e-4d90-94de-c47d28954f66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# create the model\n",
        "classifier = DecisionTreeClassifier(random_state = 0)\n",
        "\n",
        "# train the model\n",
        "classifier.fit(x_train, y_train)\n",
        "\n",
        "\n",
        "# make predictions based on the model\n",
        "predictions = classifier.predict(x_test)\n",
        "\n",
        "# evaluate the model\n",
        "round(classifier.score(x_test, y_test), 4) # evaluate the model, get predictions "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9167"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I69WlZL0BUc0",
        "colab_type": "code",
        "outputId": "81c485ce-74b3-43ac-8d50-c98ba511c168",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "classifier.tree_.max_depth  # get the depth of the tree"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-Ydi8IeB1iZ",
        "colab_type": "text"
      },
      "source": [
        "### 5. prune the tree by seting the max_depth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0DgR--HBmLR",
        "colab_type": "text"
      },
      "source": [
        "The current tree depth is 5. Let's prune it, and set the parameter max_depth to 4, and check the score\n",
        "\n",
        "Note: Since the max_depth is 5, not too big. There is no overfitting. Decrease the max_depth may not increase the score for this case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uo8ILmByB8SR",
        "colab_type": "code",
        "outputId": "381694d1-ece3-4f48-a29b-8f7ab4d14203",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "classifier = DecisionTreeClassifier(max_depth = 4, random_state = 0)\n",
        "\n",
        "# train the model\n",
        "classifier.fit(x_train, y_train)\n",
        "\n",
        "\n",
        "# make predictions based on the model\n",
        "predictions = classifier.predict(x_test)\n",
        "\n",
        "# evaluate the model\n",
        "round(classifier.score(x_test, y_test), 4) # evaluate the model, get predictions "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9167"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLF17wEfGCWH",
        "colab_type": "text"
      },
      "source": [
        "### 6. visualize the score with different max_depth"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Gx3MjMIGJb0",
        "colab_type": "code",
        "outputId": "e04f2e0c-2177-422d-f4d1-e111bb0bf55f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        }
      },
      "source": [
        "scores = []\n",
        "for i in range(1, 11):\n",
        "  classifier = DecisionTreeClassifier(max_depth = i, random_state = 0)\n",
        "  classifier.fit(x_train, y_train)\n",
        "  scores.append(classifier.score(x_test, y_test)) # evaluate the model, get predictions \n",
        "\n",
        "plt.plot(range(1, 11), scores)\n",
        "plt.title(\"Decision Tree with different max_depth\")\n",
        "plt.xlabel('Max_depth')\n",
        "plt.ylabel('Score')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEXCAYAAABcRGizAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3XucXHV9//HXO5v7PZAAIXfu4RZI\nUtRilRZBpBaKtRrUFnxYtf4EFbGKl4KForYPhNqWKkgRFQUjao01lauACGh2lwRIICGGzGZzgVwm\n98Amu5/fH+csmSy7O7ObnT0zu+/n4zGPPffzOWdmz2e+3++c81VEYGZm1pkBWQdgZmaVz8nCzMyK\ncrIwM7OinCzMzKwoJwszMyvKycLMzIpysuhHJP2fpEtKWG6npKN6I6ZKIqkmPfapnSzTKOmsbm7/\nGElRMH6fpPcXjH9N0mZJjen4u9P97ZR0Snf22Z9Iepuk1WXa9gHvXX80MOsA7EDph/1wYB/QDCwD\nvgfcGhEtB7PtiHhHicuNPJj9tEfSzoLR4cCrJMcH8NGI+EFP77OrIqIZeO3YJd0JrIyIL5dpf+cW\n7GsG8AlgakRsSid/neTc/LIc+++MpH8GJkfEpb2970qQJuwPRMTDWcdSKZwsKtNfRMQDksYAbwW+\nAbwB+GC2YXVfYQJKE+LfRcQDHS0vaWBE7OuN2CrENODl1kQhaQAwBVjanY31w/Nn5RYRflXQC1gN\nvK3NtDOAFuDkdHwIcAPQALwEfAsYVrD8hcBiYDvwB+C8dPrDJBdpgGOAR4BtwCbgRwXrB3BMOjyG\npGSzEcgBXwIGpPMuBR5LY8kDLwLv6OYx/jPwI+AuYEe67QHAF9Jj2ATcDYwrWOdM4Elga3q8b+lg\nfx8GflYw/iJwV8H4euBkki9PAUwH/h+wF2gCdrauDzQCnwaeSc/dXcCQDvZbA9wEbAZWAZcl/3Kv\nzX8sPc7zgD3pe7wT+E76N4BdwPJ0+cnAz9L34kXg4909f+n7H8Dfpse0EbgqnffO9Lj3pnHUdXB8\njcBngGfT5W4lKRXfS/LZuw8Ymy47ALgH2JC+Xw8DMws+z88AH0vHB6bv6xeKfI6GA98n+ewtBT4H\nrC6YX8r5+nF6vmqBU9J5d6XvxZ70uD7d2fnqL6/MA/CrzRvSzoU0nd5Q8M90E7AAOAQYBfwC+Go6\n7wySi9g56T/oJOCEdN7D7E8WdwFfTJcZCry5YF+FyeJ7wM/T/UwHVgAfSuddml5QPkxyYfwYsA5Q\nV48x/edtAv4ijWkYcCXw2/QYhgK3Ad9Pl59CchF+e7r8eSQXxEPb2d9x6bJK11sNNBTM25TOey1Z\npPPuBL7cZluNJBeyI4BD0/Pxdx0c52UkF7HJ6bKP0k6ySIffxoEXuraxDCBJiF8ABqcXr9XA2d08\nf60Xv2+l82aTVA0eW7C9O4q8j43p9g9Lj3EzyUV3VrrNR4AvFsR/afo5Ggr8J1BbsK1ZJBf944Br\n0u3WFNn/DSSf6XEkJbNlreewxPO1F7gIGARcBawEBhYc21kF++r0fPWHV+YB+NXmDek4WTxJcnEX\nybfNowvmvQl4MR2+Bbipg20/zP5k8T2Sb4KT21ku0n+OmvQCdGLBvI8CD6fDl5LU6bfOG56ue0RX\njzH9532ozbQXgLcWjE8BXkkvBF8EvtNm+QeB93ewz/XAqcAHgP8C6tNj/DDw03SZUpPFvILxG4H/\n7GCfj1KQSIDz6X6yOBNY1Wb7/wh8u5vnr/Xid0TB/Hrg3QXbu6PI+9gIvLdg/OfAfxSMXwHc08G6\n49P9jyiY9jngOWALcFQJ/ysNhZ8jktLg6i6cr8cK5tUALwNvKji2swrmd3q++sPLbRbVYxLJP9EE\nkotynaTWeSL5sENyQVhYwvY+C1wH/F5SHvh6RNzeZpnxJN+6cgXTcmksrTa0DkTE7jSm7jaQr2kz\nPhX4haS2DfuHkXyTvFjSRQXTBwG/6mDbjwBnkVQ3PUhy0Xxr+nqki3FuKBjeTVLCa8+RHHhMuQ6W\nK8U0YKqkrQXTaki+ALTqyvkDICLaHktX37uXCob3tDM+EpJfmgFfBd5N8rlqjWk8yZcfgDtIPpN3\nRcSqEvY9kY7Pb5fOV0Q0S1pL8p51qAfOV9VysqgCkv6I5AL9GEmVyR7gpIhY287ia4Cji20z/dB/\nON3+m4EHJD0aESsLFttEUlRvLeJDcgFqb789IdqMNwLvi4jftV1Q0hqSksXHStz2IyRVczOBq0mS\nxV+RJIsbSoynq9aTJO9WHf4ktwRrgBciYmYny3Tl/B1TZH8He+xt/S1JyerPSC7qh5LU+6tgmW8C\n/wO8U9IbI+LJItvcQHJ+l6fjhee3lPP12nuT/qBgEkk1KvT88Vc932dRwSSNlvROkobJOyPimUh+\nPvtt4CZJh6XLTZL09nS1/wY+KOlsSQPSeSe0s+2/ljQ5Hc2T/HMc8A00kp+SzgeulzRK0jSSxr47\ny3C47fkW8JXW+x4kHSbpgnTe94GLJJ2T3h8xVNKfSurom+EjJFU9ShPlo8AFJN8Mn+5gnZeAg7nf\nZD7wqfQ9OJSkmqW7ngCaJF2ZHmuNpFMkzelknc7OXzEvAdNVUHw9SKNI6vg3k5SMry+cKemDJKW+\nS0mqr74vaUSRbc4HviBpbHqMlxXMK+V8nSHpQkmDSBrqdwCL0nkH+973OU4WlekXknaQfDv6Ikm9\neOHPZj9H0hj3pKTtwAPA8QAR8ft02ZtIGrofISkZtPVHwO/S+x8WAJ/soOh/OUk1wSqSks0PgbbV\nVeVyI0m10oPp+XicJG4iYjVJ4+Q/knxDbSBp0G33Mx0Ry0hKE79Jx/MkbSePRcf3r9wGzJKUl3RP\nN+L/JkmV1zMkF6HubAOASH4Gez7JDxhWk5T6bgFGd7Jah+evBD8iaRjeIun33Qy70HdIvrWvI2n0\nf7x1hqTpJPeU/G1E7I6I75Ek8I5KfK2uISm9rQb+j6QdDij5fP2MpA1rC/Be4F2x/+fGXwH+SdJW\nSZ/q6sH2RUobaszM+o3+ftNhd7hkYWZmRTlZmFnFSp+ftbOd12ezjq2/cTWUmZkV5ZKFmZkV1Wfu\nsxg/fnxMnz496zDMzKpKXV3dpoiYUGy5PpMspk+fTm1tbdZhmJlVFUklPVnA1VBmZlaUk4WZmRXl\nZGFmZkU5WZiZWVFOFmZmVpSThZmZFeVkYWZmRfWZ+yysZ2x/ZS8Ln17P3uaOntptZpVm/MghvOOU\niWXdh5OFvSYiuPyHT/HIio1Zh2JmXXDalLFOFtZ7fvj7Bh5ZsZEv/flM/vL0ScVXMLOKMHBAT3Vo\n2Mk+yr4Hqwq5zbu4/pfP8SfHjudDb55Bz/WmaWZ9gRu4jeaW4DM/XkLNAPGv7z7VicLMXsclC+O/\nH1vFotV5bnzPLCaOGZZ1OGZWgVyy6OeWb9jBDfeu4O0nHc5Fbqcwsw44WfRjTfta+PT8xYwaOpCv\nXHSKq5/MrEOuhurH/vOhF1i6bju3/s0cDh05JOtwzKyCuWTRTy1Zs5WbH/4DfzV7MueedETW4ZhZ\nhXOy6Ide2dvMFfMXc/ioIVxzwYlZh2NmVcDVUP3Qv/zqeVZt3MUP/u4NjB46KOtwzKwKuGTRzzz+\nh01857erueRN0zjzmPFZh2NmVaKsyULSeZKWS1op6ap25k+T9KCkpyU9LGlywbxLJL2Qvi4pZ5z9\nxY5X9vIPP36aGeNHcNU7ZmYdjplVkbIlC0k1wM3AO4ATgYslta0gvwH4XkScClwLfDVd9xDgGuAN\nwBnANZLGlSvW/uK6/13G+m17+Pp7ZjFscE3W4ZhZFSlnyeIMYGVErIqIJuBu4MI2y5wIPJQO/7pg\n/tuB+yNiS0TkgfuB88oYa5/3wLKXmF/byMfOOprZU513zaxrypksJgFrCsYb02mFlgDvSocvAkZJ\nOrTEdZH0EUm1kmo3bvRjtTuyZVcTV/30GWZOHM0nzz4u63DMrApl3cD9GeCtkp4C3gqsBZpLXTki\nbo2IuRExd8KECeWKsapFBF/6n2fYtqeJG98zi8EDs37LzawalfPKsRaYUjA+OZ32mohYFxHviojT\ngS+m07aWsq6VZsGSdSx8ZgNXnHMcMyeOzjocM6tS5UwWi4BjJc2QNBiYBywoXEDSeEmtMXweuD0d\nvhc4V9K4tGH73HSadcGGba/wj//zLLOnjuWjbzk663DMrIqVLVlExD7gMpKL/HPA/IhYKulaSRek\ni50FLJe0AjgcuD5ddwtwHUnCWQRcm06zEkUEn/3J0+xtDr7+ntOo6YWetMys7yrrHdwRsRBY2Gba\n1QXD9wD3dLDu7ewvaVgX/eB3DTy6YiPXXXgSM8aPyDocM6tybu3sg3Kbd/GVhUkXqR9447SswzGz\nPsDJoo9pbgmunO8uUs2sZ/lBgn3Mbb9ZRW3OXaSaWc9yyaIPWb5hB1+/z12kmlnPc7LoI5r2tXDF\njxYzepi7SDWznudqqD7iPx56gWXr3UWqmZWHSxZ9wOI1W/kvd5FqZmXkZFHlXtnbzKfdRaqZlZmr\noaqcu0g1s97gkkUVe3ylu0g1s97hZFGltr+yl3+452mOchepZtYLXA1Vpa77RdJF6j0f+2N3kWpm\nZeeSRRV6YNlL/LjOXaSaWe9xsqgym3e+ylU/fdpdpJpZr3KyqCJJF6nPsm3PXneRama9ylebKvLz\nxev4v2fdRaqZ9T4niyqxYdsrXP3zZ5kzbZy7SDWzXudkUQUO6CL1r2e5i1Qz63VOFlWgtYvUL5x/\nAtPdRaqZZcDJosKt3rSL63/pLlLNLFtOFhWsuSX4zI+XMLDGXaSaWbZ8B3cF+7a7SDWzCuGSRYV6\nfsN2brxvBeeddIS7SDWzzDlZVKCmfS18+kdLGD1sINdfdLKrn8wsc66GqkD//qC7SDWzyuKSRYV5\nqiHPfz280l2kmllFcbKoIHuamrly/hKOGD3UXaSaWUVxNVQF+ZdfPc+qTe4i1cwqj0sWFWLZuu3c\n8fhqLv3j6e4i1cwqjpNFhfjNCxsB+PifHpNxJGZmr+dkUSHqcnmmHTqcCaP86yczqzxOFhUgIqjL\n5ZkzzV2kmlllcrKoALnNu9m8q8nJwswqlpNFBajL5QGcLMysYjlZVIDaXJ5RQwZy3GGjsg7FzKxd\nZU0Wks6TtFzSSklXtTN/qqRfS3pK0tOSzk+nT5e0R9Li9PWtcsaZtfpcntOnjWOAe8AzswpVtpvy\nJNUANwPnAI3AIkkLImJZwWJfAuZHxDclnQgsBKan8/4QEaeVK75KsW3PXla8vIM/P3Vi1qGYmXWo\nnCWLM4CVEbEqIpqAu4EL2ywTwOh0eAywrozxVKSnGvJEuL3CzCpbOZPFJGBNwXhjOq3Ql4EPSGok\nKVVcXjBvRlo99YikPyljnJmqz+UZIJg1ZWzWoZiZdSjrBu6LgTsiYjJwPvB9SQOA9cDUiDgd+DTw\nQ0mj264s6SOSaiXVbty4sVcD7ym1uTwzJ45m5BA/psvMKlc5k8VaYErB+OR0WqEPAfMBIuIJYCgw\nPiJejYjN6fQ64A/AcW13EBG3RsTciJg7YcKEMhxCee1rbmHxmq2ugjKzilfOZLEIOFbSDEmDgXnA\ngjbLNABnA0iaSZIsNkqakDaQI+ko4FhgVRljzcTzG3awu6nZycLMKl7Z6j4iYp+ky4B7gRrg9ohY\nKulaoDYiFgBXAt+WdAVJY/elERGS3gJcK2kv0AL8fURsKVesWfHNeGZWLcpaUR4RC0kargunXV0w\nvAw4s531fgL8pJyxVYK6XJ7DRw9h0thhWYdiZtaprBu4+7W6XJ650w5B8s14ZlbZnCwysn7bHtZu\n3cNsV0GZWRVwsshIa3vFXCcLM6sCThYZqcvlGTpoACce+brbR8zMKo6TRUbqc3lOnTyWQTV+C8ys\n8vlKlYE9Tc0sXbfdVVBmVjWcLDKwpHEr+1rC91eYWdVwsshAa+P27KlOFmZWHZwsMlCXy3P0hBGM\nGzE461DMzEriZNHLWlqC+oa8q6DMrKo4WfSyVZt2snX3XuZOOyTrUMzMSuZk0ctea69wycLMqoiT\nRS+rXZ1n7PBBHD1hRNahmJmVzMmil9U15JkzdZwfHmhmVcXJohdt2dXEqo27XAVlZlXHyaIX1fvh\ngWZWpZwselFdQ56BA8Spk8dmHYqZWZc4WfSiulyekyaNYdjgmqxDMTPrEieLXtK0r4Ula7Yyx4/4\nMLMq5GTRS5at386r+1p857aZVSUni15Su3oLAHOnO1mYWfVxsugl9Q15Jo0dxuGjh2YdiplZlzlZ\n9IKIoC6Xd6nCzKpWyclC0pslfTAdniBpRvnC6lsa83t4afurbq8ws6pVUrKQdA3wOeDz6aRBwJ3l\nCqqvqW9wZ0dmVt1KLVlcBFwA7AKIiHXAqHIF1dfUrs4zYnANJxzhU2Zm1anUZNEUEQEEgCQ/MrUL\n6nJ5Tps6loE1biIys+pU6tVrvqRbgLGSPgw8AHy7fGH1HTtf3cfzG7Yzx50dmVkVG1jKQhFxg6Rz\ngO3A8cDVEXF/WSPrIxY3bKUlcOO2mVW1oslCUg3wQET8KeAE0UV1uTwSnD7VDw80s+pVtBoqIpqB\nFkljeiGePqc2t4XjDx/F6KGDsg7FzKzbSqqGAnYCz0i6n/QXUQAR8YmyRNVHNLcEixu28henHZl1\nKGZmB6XUZPHT9GVd8MLLO9jx6j53dmRmVa/UBu7vShoMHJdOWh4Re8sXVt9Quzq5Gc+N22ZW7UpK\nFpLOAr4LrAYETJF0SUQ8Wr7Qql99Ls/4kYOZesjwrEMxMzsopVZDfR04NyKWA0g6DrgLmFOuwPqC\n2lyeOdPGISnrUMzMDkqpN+UNak0UABGxguT5UJ2SdJ6k5ZJWSrqqnflTJf1a0lOSnpZ0fsG8z6fr\nLZf09hLjrBgv73iFhi27XQVlZn1CqSWLWkm3sf/hge8HajtbIb0/42bgHKARWCRpQUQsK1jsS8D8\niPimpBOBhcD0dHgecBJwJPCApOPSn/FWhfrcVsDtFWbWN5RasvgYsAz4RPpalk7rzBnAyohYFRFN\nwN3AhW2WCWB0OjwGWJcOXwjcHRGvRsSLwMp0e1WjLreFwTUDOHmSb08xs+pXasliIPCNiLgRXis1\nDCmyziRgTcF4I/CGNst8GbhP0uXACOBtBes+2WbdSW13IOkjwEcApk6dWspx9Jq6XJ5TJo9hyMCa\nrEMxMztopZYsHgSGFYwPI3mY4MG6GLgjIiYD5wPfl1Tyo1kj4taImBsRcydMmNAD4fSMV/Y28+za\n7b6/wsz6jFJLFkMjYmfrSETslFTs96BrgSkF45PTaYU+BJyXbvMJSUOB8SWuW7GeXbuNpuYWZjtZ\nmFkfUeq3+F2SZreOSJoL7CmyziLgWEkz0hv65gEL2izTAJydbnMmMBTYmC43T9KQtPvWY4Hflxhr\n5upy7hnPzPqWUksWnwJ+LKm1AXoi8N7OVoiIfZIuA+4FaoDbI2KppGuB2ohYAFwJfFvSFSSN3Zem\nnSwtlTSfpCF9H/DxavolVG0uz/RDhzNhVLFmHTOz6tBpspD0R8CaiFgk6QTgo8C7gF8BLxbbeEQs\nJPk5bOG0qwuGlwFndrDu9cD1xfZRaSKC+lyetx5fOW0oZmYHq1g11C1AUzr8JuALJPdO5IFbyxhX\n1Vq9eTebdzUx1z3jmVkfUqwaqiYitqTD7wVujYifAD+RtLi8oVWn1vYK34xnZn1JsZJFjaTWhHI2\n8FDBvFLbO/qVulyeUUMHcuxhI7MOxcysxxS74N8FPCJpE8mvn34DIOkYYFuZY6tKdbktzJ46jgED\n/PBAM+s7Ok0WEXG9pAdJfv10X/pLJUhKJJeXO7hqs23PXla8tJN3nuqe8cysbylalRQRT7YzbUV5\nwqlu9Q1Je4Xv3DazvqbkR2tYcfW5PAMEs6aMzToUM7Me5WTRg+pyeWZOHM2IIW77N7O+xcmih+xr\nbmHxmq2ugjKzPsnJooc8v2EHu5ua/fBAM+uTnCx6SO3q5N7FudN957aZ9T1OFj2krmErR4weypFj\nhmYdiplZj3Oy6CH1uTxzpo1D8s14Ztb3OFn0gPXb9rB26x4/D8rM+iwnix7ghweaWV/nZNEDalfn\nGTpoACceOTrrUMzMysLJogfUN+SZNXksg2p8Os2sb/LV7SDtbtrH0nXbXQVlZn2ak8VBWrJmG80t\nwdzpThZm1nc5WRyk1ifNzp7qZGFmfZeTxUGqy+U55rCRjB0+OOtQzMzKxsniILS0BHW5PHNcqjCz\nPs7J4iCs2rSTbXv2unHbzPo8J4uDULs6vRnPjdtm1sc5WRyEulyeccMHcdT4EVmHYmZWVk4WB6Gu\nwQ8PNLP+wcmim7bsamLVxl3u7MjM+gUni26qb314oH8JZWb9gJNFN9Xm8gwcIGZNGZt1KGZmZedk\n0U31uTwnTRrD0EE1WYdiZlZ2Thbd0LSvhSWNW5nr9goz6yecLLph6bptvLqvxTfjmVm/4WTRDe4Z\nz8z6GyeLbqjL5Zk8bhiHjx6adShmZr3CyaKLIoLaXN6lCjPrV8qaLCSdJ2m5pJWSrmpn/k2SFqev\nFZK2FsxrLpi3oJxxdkVjfg8bd7zqxm0z61cGlmvDkmqAm4FzgEZgkaQFEbGsdZmIuKJg+cuB0ws2\nsSciTitXfN3V2l7hO7fNrD8pZ8niDGBlRKyKiCbgbuDCTpa/GLirjPH0iLpcnhGDazj+8FFZh2Jm\n1mvKmSwmAWsKxhvTaa8jaRowA3ioYPJQSbWSnpT0l+ULs2tqc3lOnzqOgTVu7jGz/qNSrnjzgHsi\norlg2rSImAu8D/g3SUe3XUnSR9KEUrtx48ayB7njlb0s37DdVVBm1u+UM1msBaYUjE9Op7VnHm2q\noCJibfp3FfAwB7ZntC5za0TMjYi5EyZM6ImYO7VkzTZaAjdum1m/U85ksQg4VtIMSYNJEsLrftUk\n6QRgHPBEwbRxkoakw+OBM4FlbdftbbW5LUhw2lQ/PNDM+pey/RoqIvZJugy4F6gBbo+IpZKuBWoj\nojVxzAPujogoWH0mcIukFpKE9rXCX1FlpS6X5/jDRzF66KCsQzEz61VlSxYAEbEQWNhm2tVtxr/c\nznqPA6eUM7auam4JnmrYyoWnHZl1KGZmva5SGrgr3oqXdrDz1X2+c9vM+iUnixK13ow3d9ohGUdi\nZtb7nCxKVJfLM37kEKYcMizrUMzMep2TRYnqcnnmTBuLpKxDMTPrdU4WJXh5xys0bNntKigz67ec\nLEpQ74cHmlk/52RRgrpcnsEDB3DypNFZh2JmlgknixLU5vKcOmkMQwbWZB2KmVkmnCyKeGVvM8+u\n3eb7K8ysX3OyKOKZtdvY2xxOFmbWrzlZFOGe8czMnCyKqsvlmX7ocMaPHJJ1KGZmmXGy6EREUJ/L\nM8f3V5hZP+dk0YnVm3ezeVeT2yvMrN9zsuhE7eotAMyd7mRhZv2bk0Un6hvyjBo6kGMmjMw6FDOz\nTDlZdKIul2f21HEMGOCHB5pZ/+Zk0YFtu/ey4qWdzHV7hZmZk0VH6tck91e4cdvMzMmiQ/W5PDUD\nxKwpY7MOxcwsc04WHahdnWfmxFGMGDIw61DMzDLnZNGOfc0tLF6zlTlTXQVlZgZOFu16bv0O9uxt\nZs5037ltZgZOFu2qyyU347lx28ws4WTRjrqGrUwcM5RJY4dlHYqZWUVwsmhH3eotfiS5mVkBJ4s2\n1m3dw7ptr7hx28ysgJNFG62dHfnhgWZm+zlZtFGXyzNsUA0zJ47OOhQzs4rhZNFGfUOeWVPGMKjG\np8bMrJWviAV2N+1j6brt/smsmVkbThYFlqzZRnNLOFmYmbXhZFGg9Wa82f4llJnZAZwsCtTl8hxz\n2EjGDh+cdShmZhXFySLV0hLUN2x1Z0dmZu1wskj9YeNOtu3Z6zu3zczaUdZkIek8ScslrZR0VTvz\nb5K0OH2tkLS1YN4lkl5IX5eUM07YfzOeG7fNzF6vbD37SKoBbgbOARqBRZIWRMSy1mUi4oqC5S8H\nTk+HDwGuAeYCAdSl6+bLFW9tLs+44YM4avyIcu3CzKxqlbNkcQawMiJWRUQTcDdwYSfLXwzclQ6/\nHbg/IrakCeJ+4Lwyxkp9Ls+caeOQVM7dmJlVpXImi0nAmoLxxnTa60iaBswAHurKupI+IqlWUu3G\njRu7HeiWXU2s2rSLOdPc2ZGZWXsqpYF7HnBPRDR3ZaWIuDUi5kbE3AkTJnR7526vMDPrXDmTxVpg\nSsH45HRae+axvwqqq+setLpcnkE14tTJY8q1CzOzqlbOZLEIOFbSDEmDSRLCgrYLSToBGAc8UTD5\nXuBcSeMkjQPOTaeVRV1uCycdOYahg2rKtQszs6pWtmQREfuAy0gu8s8B8yNiqaRrJV1QsOg84O6I\niIJ1twDXkSScRcC16bQe17SvhSWN21wFZWbWibL9dBYgIhYCC9tMu7rN+Jc7WPd24PayBZfK725i\n7rRxvOmoQ8u9KzOzqlXWZFENDh89lB9++I1Zh2FmVtEq5ddQZmZWwZwszMysKCcLMzMrysnCzMyK\ncrIwM7OinCzMzKwoJwszMyvKycLMzIpSwVM2qpqkjUAu6zgO0nhgU9ZBVBCfjwP5fOznc3Gggzkf\n0yKi6GO7+0yy6Ask1UbE3KzjqBQ+Hwfy+djP5+JAvXE+XA1lZmZFOVmYmVlRThaV5dasA6gwPh8H\n8vnYz+fiQGU/H26zMDOzolyyMDOzopwszMysKCeLCiBpiqRfS1omaamkT2YdU9Yk1Uh6StL/Zh1L\n1iSNlXSPpOclPSfpTVnHlCVJV6T/J89KukvS0Kxj6k2Sbpf0sqRnC6YdIul+SS+kf3u8n2gni8qw\nD7gyIk4E3gh8XNKJGceUtU+S9N1u8A3gVxFxAjCLfnxeJE0CPgHMjYiTgRpgXrZR9bo7gPPaTLsK\neDAijgUeTMd7lJNFBYiI9RFRnw7vILkYTMo2quxImgz8OXBb1rFkTdIY4C3AfwNERFNEbM02qswN\nBIZJGggMB9ZlHE+viohHgS1tJl8IfDcd/i7wlz29XyeLCiNpOnA68LtsI8nUvwGfBVqyDqQCzAA2\nAt9Jq+VukzQi66CyEhFrgRtO+2IJAAAD2UlEQVSABmA9sC0i7ss2qopweESsT4c3AIf39A6cLCqI\npJHAT4BPRcT2rOPJgqR3Ai9HRF3WsVSIgcBs4JsRcTqwizJUMVSLtC7+QpIkeiQwQtIHso2qskRy\nP0SP3xPhZFEhJA0iSRQ/iIifZh1Phs4ELpC0Grgb+DNJd2YbUqYagcaIaC1p3kOSPPqrtwEvRsTG\niNgL/BT444xjqgQvSZoIkP59uad34GRRASSJpE76uYi4Met4shQRn4+IyRExnaTh8qGI6LffHCNi\nA7BG0vHppLOBZRmGlLUG4I2Shqf/N2fTjxv8CywALkmHLwF+3tM7cLKoDGcCf0PyLXpx+jo/66Cs\nYlwO/EDS08BpwFcyjiczaQnrHqAeeIbkGtavHv0h6S7gCeB4SY2SPgR8DThH0gskpa+v9fh+/bgP\nMzMrxiULMzMrysnCzMyKcrIwM7OinCzMzKwoJwszMyvKycLMzIpysrB+TVIU3iEuaaCkjeV8NLqk\n1ZLGd3PdSyUd2RPbMusKJwvr73YBJ0salo6fA6zNMJ5iLiV5JpJZr3KyMIOFJI9EB7gYuKt1hqQz\nJD2RPvH18dbHbqQd8NyeDp+SdsQzvL2NSzpU0n1phz23ASqY9wFJv0/v2r9FUk06faekm9J1HpQ0\nQdK7gbkkd3MvLkhwl0uql/SMpBN6+NyYAU4WZpA8sHBe2uPaqRz4ePjngT9Jn/h6NfsftfEN4BhJ\nFwHfAT4aEbs72P41wGMRcRLwM2AqgKSZwHuBMyPiNKAZeH+6zgigNl3nEeCaiLgHqAXeHxGnRcSe\ndNlNETEb+CbwmYM5EWYdGZh1AGZZi4in035ELiYpZRQaA3xX0rEkj30elK7TIulS4Gngloj4bSe7\neAvwrnS9X0rKp9PPBuYAi5Jn4jGM/U8LbQF+lA7fSfJ01Y60zqtr3Y9ZT3OyMEssIOlU5yzg0ILp\n1wG/joiL0oTycMG8Y4GddL8NQcB3I+LzJSzb2UPcXk3/NuP/aSsTV0OZJW4H/ikinmkzfQz7G7wv\nbZ2Ydnf67ySlhkPT9oSOPAq8L13vHcC4dPqDwLslHZbOO0TStHTeAKB1m+8DHkuHdwCjunRkZj3A\nycIMiIjGiPj3dmb9K/BVSU9x4Lf2m4CbI2IF8CHga60X/Xb8E/AWSUtJqoka0n0uA74E3Jc+fvx+\nYGK6zi7gDEnPAn8GXJtOvwP4VpsGbrOy8yPKzSqQpJ0RMTLrOMxauWRhZmZFuWRh1kMkfRD4ZJvJ\nv42Ij2cRj1lPcrIwM7OiXA1lZmZFOVmYmVlRThZmZlaUk4WZmRX1/wFOZG4ZcbxjdQAAAABJRU5E\nrkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HN8MQbST_r30",
        "colab_type": "text"
      },
      "source": [
        "# Part Three: Decision Tree Limitations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KesnbBJe_4fu",
        "colab_type": "text"
      },
      "source": [
        "- 1. The current strategy of creating trees is greedy. We can only get the local optimal.\n",
        "\n",
        "It iwill find the largest information gain right now and split on that feature. We never consider the ramifications of that split futher down the tree. \n",
        "\n",
        "Finding a globally optimal tree is an extremely difficult task, and finding a tree using our greedy approach is reasonable substitute.\n",
        "\n",
        "- 2. Another problem with our trees is that they potentially overfit the data. \n",
        "\n",
        "This means that the structure of the tree is too dependent on the training data and doesn't accurately represent the way the data in the real world looks like. In general, larger trees tend to overfit the data more. As the tree gets bigger, it becomes more tuned to the training data and it loses a more generalized understanding of the real world data\n",
        "\n",
        "One way to solve the overfitting problem is to prune the tree. The goal of pruning is to shrink the size of the tree. There are a few different pruning strategies. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFO5Foh3vtot",
        "colab_type": "text"
      },
      "source": [
        "## Other notes: transform string variable into numerical"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3iCFwWbIh3V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#data['sex_int'] = data['sex'].apply(lambda row:0 if row == 'Male' else 1)\n",
        "\n",
        "#data[\"country-int\"] = data['native-country'].apply(lambda row : 0 if row == \"United-States\" else 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGBuD6xlv8ZY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}