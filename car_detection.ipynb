{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "car detection.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xslittlemaggie/Deep-Learning-Machine-Learning-Projects/blob/master/car_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dmzr1QAesaEf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIj-5_pmB9dr",
        "colab_type": "text"
      },
      "source": [
        "## Step 0: Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DpMjCSFQJOR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "69dfba9e-5256-4f80-a485-4fdb71fbbca8"
      },
      "source": [
        "from keras.models import Model, load_model\n",
        "from keras.layers import Input, BatchNormalization, Activation, Dense, Dropout\n",
        "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
        "from keras.layers.pooling import MaxPooling2D, GlobalMaxPool2D\n",
        "from keras.layers.merge import concatenate, add\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from keras.optimizers import Adam\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "from keras.preprocessing import image\n",
        "\n",
        "# graphs\n",
        "% matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import cv2\n",
        "import os\n",
        "import glob"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQFi-oBVCBj8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svoPLT_9lDCK",
        "colab_type": "text"
      },
      "source": [
        "## Step1: Upload data from google drive to colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtBtyCiJGM0Q",
        "colab_type": "code",
        "outputId": "31fa2a37-ca0f-4ad9-da13-b21fb8ac3844",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOitALFGEaKV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# After executing the cell above, Drive\n",
        "# files will be present in \"/content/drive/My Drive\".\n",
        "# !ls \"/content/drive/My Drive\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AM0iHnersjli",
        "colab_type": "text"
      },
      "source": [
        "## Step 1: Get familar with the data & visualize the images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hsn3-4M1tq6L",
        "colab_type": "text"
      },
      "source": [
        "#### 1.1 visualize one image with the corresponding labelled image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-In8pWettz8j",
        "colab_type": "code",
        "outputId": "9fe36054-ae7d-406f-8615-e7a577ee1513",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 740
        }
      },
      "source": [
        "#fig = plt.figure(figsize = (8, 6))\n",
        "img_RGB = \"/content/drive/My Drive/Deep Learning projects/car segmentation/training data part/episode_0000/CameraRGB/000020.png\"\n",
        "img_Seg = \"/content/drive/My Drive/Deep Learning projects/car segmentation/training data part/episode_0000/CameraSeg/000020.png\"\n",
        "img_RGB_arary = cv2.imread(img_RGB)\n",
        "img_Seg_arary = cv2.imread(img_Seg)\n",
        "plt.rcParams['figure.figsize'] = (8, 6)\n",
        "plt.imshow(img_RGB_arary)\n",
        "plt.show()\n",
        "plt.imshow(img_Seg_arary[:, :, 0])\n",
        "plt.show() \n",
        " "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-a37d971bdf58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mimg_Seg_arary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_Seg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrcParams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'figure.figsize'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_RGB_arary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_Seg_arary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, data, **kwargs)\u001b[0m\n\u001b[1;32m   2697\u001b[0m         \u001b[0mfilternorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilternorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilterrad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilterrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimlim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimlim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2698\u001b[0m         resample=resample, url=url, **({\"data\": data} if data is not\n\u001b[0;32m-> 2699\u001b[0;31m         None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2700\u001b[0m     \u001b[0msci\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__ret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2701\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m__ret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1808\u001b[0m                         \u001b[0;34m\"the Matplotlib list!)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlabel_namer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m                         RuntimeWarning, stacklevel=2)\n\u001b[0;32m-> 1810\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1812\u001b[0m         inner.__doc__ = _add_data_doc(inner.__doc__,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5492\u001b[0m                               resample=resample, **kwargs)\n\u001b[1;32m   5493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5494\u001b[0;31m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5495\u001b[0m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5496\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mset_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    632\u001b[0m         if (self._A.dtype != np.uint8 and\n\u001b[1;32m    633\u001b[0m                 not np.can_cast(self._A.dtype, float, \"same_kind\")):\n\u001b[0;32m--> 634\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Image data cannot be converted to float\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m         if not (self._A.ndim == 2\n",
            "\u001b[0;31mTypeError\u001b[0m: Image data cannot be converted to float"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXMAAAFpCAYAAABu98hvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngJgDDCDmAAOIOcAAYg4wgJgDDPB/bPOaG9rG2MUAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 576x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DaZcK5_0pFIF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_Seg_arary.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7gs_51KyKnl",
        "colab_type": "text"
      },
      "source": [
        "## Step 2: Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQoCC0TaK_F4",
        "colab_type": "text"
      },
      "source": [
        "#### 2.1 Combine episode_0001 to episode_0014 for training & testing dataset respectively"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVCamqFVLyP2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create two files, x_train, y_train to store the original images and labels\n",
        "#os.mkdir(\"/content/drive/My Drive/Deep Learning projects/car segmentation/training data part/x_train_images\")\n",
        "#os.mkdir(\"/content/drive/My Drive/Deep Learning projects/car segmentation/training data part/y_train_labels\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adDF1ViRNqRQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# move all RGB images to x_train_images\n",
        "episodes = [\"0001/\", \"0002/\", \"0003/\", \"0004/\", \"0005/\", \"0006/\", \"0007/\", \"0008/\", \"0009/\", \"0010/\", \"0011/\", \"0012/\", \"0013/\", \"0014/\"]\n",
        "for episode in episodes:\n",
        "  pathes = glob.glob(\"/content/drive/My Drive/Deep Learning projects/car segmentation/training data part/episode_\" + episode + \"CameraRGB/*.png\")\n",
        "  for path in pathes:\n",
        "    head, tail = os.path.split(path)\n",
        "    new_path = \"/content/drive/My Drive/Deep Learning projects/car segmentation/training data part/x_train_images/\" + head[-22:-10] + \"RGB_\" + tail\n",
        "    os.rename(path, new_path)\n",
        "    #print(new_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vM5iTpluW4L7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# move all Seg (labels) to y_train_images\n",
        "episodes = [\"0001/\", \"0002/\", \"0003/\", \"0004/\", \"0005/\", \"0006/\", \"0007/\", \"0008/\", \"0009/\", \"0010/\", \"0011/\", \"0012/\", \"0013/\", \"0014/\"]\n",
        "for episode in episodes:\n",
        "  pathes = glob.glob(\"/content/drive/My Drive/Deep Learning projects/car segmentation/training data part/episode_\" + episode + \"CameraSeg/*.png\")\n",
        "  for path in pathes:\n",
        "    head, tail = os.path.split(path)\n",
        "    new_path = \"/content/drive/My Drive/Deep Learning projects/car segmentation/training data part/y_train_labels/\" + head[-22:-10] + \"Seg_\" + tail\n",
        "    os.rename(path, new_path)\n",
        "    #print(new_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9uGxMNDZbTf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "607d69a0-837b-4d90-e0c7-b0b65ebbfa9b"
      },
      "source": [
        "dir_train_images = '/content/drive/My Drive/Deep Learning projects/car segmentation/training data part/x_train_images'\n",
        "dir_train_labels = '/content/drive/My Drive/Deep Learning projects/car segmentation/training data part/y_train_labels'\n",
        "\n",
        "dir_train_images_fname = os.listdir(dir_train_images)\n",
        "dir_train_labels_fname = os.listdir(dir_train_labels)\n",
        "\n",
        "print(\"Total training images:\", len(dir_train_images_fname))\n",
        "print(\"Total training labels:\", len(dir_train_labels_fname))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total training images: 2523\n",
            "Total training labels: 2520\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ry5Lg8qmFBn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbg1s_1hLALK",
        "colab_type": "text"
      },
      "source": [
        "#### 2.2 Samll dataset (x_train, y_train ) episod_0000 for pilot practice"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ju5UhscQig8f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "b440bc42-999f-4c5a-bf7b-50109eb3a730"
      },
      "source": [
        "os.mkdir(\"/content/drive/My Drive/Deep Learning projects/car segmentation/training data part/episode_0000/CameraRGB/training_imgs\")\n",
        "os.mkdir(\"/content/drive/My Drive/Deep Learning projects/car segmentation/training data part/episode_0000/CameraSeg/training_labels\")\n",
        "\n",
        "os.mkdir(\"/content/drive/My Drive/Deep Learning projects/car segmentation/validation data part\")       \n",
        "         \n",
        "os.mkdir(\"/content/drive/My Drive/Deep Learning projects/car segmentation/validation data part/validation_imgs\")\n",
        "os.mkdir(\"/content/drive/My Drive/Deep Learning projects/car segmentation/validation data part/validation_imgs/data\")\n",
        "os.mkdir(\"/content/drive/My Drive/Deep Learning projects/car segmentation/validation data part/validation_labels\")\n",
        "os.mkdir(\"/content/drive/My Drive/Deep Learning projects/car segmentation/validation data part/validation_labels/data\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileExistsError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-934f99b2d89b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/My Drive/Deep Learning projects/car segmentation/training data part/episode_0000/CameraRGB/training_imgs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/My Drive/Deep Learning projects/car segmentation/training data part/episode_0000/CameraSeg/training_labels\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/My Drive/Deep Learning projects/car segmentation/validation data part\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: '/content/drive/My Drive/Deep Learning projects/car segmentation/training data part/episode_0000/CameraRGB/training_imgs'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQNf23qRi0Kg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Move images and labels to the sub_files\n",
        "\"\"\"\n",
        "pathes = glob.glob(\"/content/drive/My Drive/Deep Learning projects/car segmentation/training data part/episode_0000/CameraRGB/*.png\")\n",
        "for path in pathes[:150]:\n",
        "  head, tail = os.path.split(path)\n",
        "  #print(head)\n",
        "  #print(tail)\n",
        "  new_path = head + '/training_imgs/' + tail\n",
        "  os.rename(path, new_path)\n",
        "\"\"\"\n",
        "# Move the rest 30 images for validation\n",
        "for path in pathes[150:]:\n",
        "  head, tail = os.path.split(path)\n",
        "  #print(head)\n",
        "  #print(tail)\n",
        "  #print(head)\n",
        "  #print(tail)\n",
        "  new_path = \"/content/drive/My Drive/Deep Learning projects/car segmentation/validation data part/validation_imgs/data/\" + tail\n",
        "  os.rename(path, new_path)\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xRuMDR5m1ao",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Move images and labels to the sub_files\n",
        "pathes = glob.glob(\"/content/drive/My Drive/Deep Learning projects/car segmentation/training data part/episode_0000/CameraSeg/*.png\")\n",
        "for path in pathes:\n",
        "  head, tail = os.path.split(path)\n",
        "  #print(head)\n",
        "  #print(tail)\n",
        "  new_path = head + '/training_labels/' + tail\n",
        "  os.rename(path, new_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4LGnY6xeEHe",
        "colab_type": "text"
      },
      "source": [
        "#### Data Augumentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1yv7302cjYq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "808aeef4-cc27-4b47-d7b7-40d3c2ac2da3"
      },
      "source": [
        "# Creating the training Image and Mask generator\n",
        "\n",
        "batch_size = 16\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale = 1/255.) # shear_range = 0.5, rotation_range=50, zoom_range=0.2, width_shift_range=0.2, height_shift_range=0.2, fill_mode='reflect'\n",
        "\n",
        "#mask_datagen = image.ImageDataGenerator()\n",
        "\n",
        "train_image_generator = train_datagen.flow_from_directory(\"/content/drive/My Drive/Deep Learning projects/car segmentation/training data part/episode_0000/CameraRGB\", \n",
        "                       batch_size = batch_size) # target_size = (img_size, img_size), shuffle = True\n",
        "\n",
        "train_mask_generator = train_datagen.flow_from_directory(\"/content/drive/My Drive/Deep Learning projects/car segmentation/training data part/episode_0000/CameraSeg\", \n",
        "                       batch_size = batch_size) # target_size = (img_size, img_size), shuffle = True\n",
        "\n",
        "# Creating the validation Image and Mask generator\n",
        "val_datagen = ImageDataGenerator(rescale = 1/255.)\n",
        "\n",
        "val_image_generator = val_datagen.flow_from_directory(\"/content/drive/My Drive/Deep Learning projects/car segmentation/validation data part/validation_imgs\",\n",
        "                      batch_size = batch_size)\n",
        "val_mask_generator = val_datagen.flow_from_directory(\"/content/drive/My Drive/Deep Learning projects/car segmentation/validation data part/validation_labels\",\n",
        "                      batch_size = batch_size)\n",
        "\n",
        "\n",
        "train_generator = zip(train_image_generator, train_mask_generator)\n",
        "val_generator = zip(val_image_generator, val_mask_generator)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 180 images belonging to 1 classes.\n",
            "Found 180 images belonging to 1 classes.\n",
            "Found 0 images belonging to 1 classes.\n",
            "Found 0 images belonging to 1 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8SlDAxLoo9G",
        "colab_type": "text"
      },
      "source": [
        "#### Creating data generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7a8ZzjKonIx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def data_gen(img_folder, mask_folder, batch_size):\n",
        "  c = 0\n",
        "  n = os.listdir(img_folder) # List of training images\n",
        "  random.shuffle(n)\n",
        "  \n",
        "  while (True):\n",
        "    img = np.zeros((batch_size, 256, 800, 3)).astype('float')\n",
        "    mask = np.zeros((batch_size, 256, 800, 1)).astype('float')\n",
        "    \n",
        "    for i in range(c, c + batch_size): # initially from 0 to 16, c = 0\n",
        "      train_img = cv2.imread(img_folder + '/' + n[i])/255.\n",
        "      #train_img = cv2.resize(train_img, (512, 512)) # Read an image from folder and resize the image\n",
        "      \n",
        "      img[i-c] = train_img # add to array -img[0], img[1], and so on\n",
        "      \n",
        "      train_mask = cv2.imread(mask_folder + '/' + n[i], cv2.IMREAD_GRAYSCALE)/255.\n",
        "      #train_mask = cv2.resize(train_mask, (512, 512))\n",
        "      train_mask = train_mask.reshape(256, 800, 1) # Add extra dimension for the parity with train_img\n",
        "      \n",
        "      mask[i-c] = train_mask\n",
        "      \n",
        "    c += batch_size\n",
        "    if (c + batch_size >= len(os.listdir(img_folder))):\n",
        "      c = 0\n",
        "      random.shuffle(n)\n",
        "    yield img, mask\n",
        "    \n",
        "train_frame_path = '/content/drive/My Drive/Deep Learning projects/car segmentation/training data part/episode_0000/CameraRGB/training_imgs'\n",
        "train_mask_path = '/content/drive/My Drive/Deep Learning projects/car segmentation/training data part/episode_0000/CameraSeg/training_labels'\n",
        "\n",
        "val_frame_path = '/content/drive/My Drive/Deep Learning projects/car segmentation/validation data part/validation_imgs/data'\n",
        "val_mask_path = '/content/drive/My Drive/Deep Learning projects/car segmentation/validation data part/validation_labels/data'\n",
        "\n",
        "# train the model\n",
        "train_gen = data_gen(train_frame_path, train_mask_path, batch_size = 4)\n",
        "val_gen = data_gen(val_frame_path, val_mask_path, batch_size = 4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjndR0nnr4EU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "35661fe2-e207-44ca-bedd-dab68a2433cd"
      },
      "source": [
        "n = os.listdir('/content/drive/My Drive/Deep Learning projects/car segmentation/training data part/episode_0000/CameraRGB/training_imgs') # List of training images\n",
        "print(n)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['000020.png', '000021.png', '000022.png', '000025.png', '000023.png', '000027.png', '000024.png', '000026.png', '000028.png', '000030.png', '000029.png', '000031.png', '000032.png', '000035.png', '000034.png', '000036.png', '000033.png', '000037.png', '000040.png', '000038.png', '000041.png', '000039.png', '000042.png', '000043.png', '000044.png', '000045.png', '000046.png', '000050.png', '000047.png', '000051.png', '000048.png', '000052.png', '000049.png', '000054.png', '000053.png', '000056.png', '000057.png', '000060.png', '000055.png', '000062.png', '000058.png', '000061.png', '000059.png', '000063.png', '000064.png', '000065.png', '000066.png', '000070.png', '000067.png', '000071.png', '000068.png', '000072.png', '000069.png', '000074.png', '000073.png', '000075.png', '000080.png', '000076.png', '000081.png', '000077.png', '000082.png', '000078.png', '000079.png', '000083.png', '000084.png', '000085.png', '000086.png', '000090.png', '000088.png', '000091.png', '000087.png', '000092.png', '000089.png', '000093.png', '000094.png', '000096.png', '000104.png', '000097.png', '000100.png', '000095.png', '000101.png', '000099.png', '000102.png', '000098.png', '000103.png', '000105.png', '000110.png', '000107.png', '000111.png', '000106.png', '000112.png', '000109.png', '000108.png', '000113.png', '000114.png', '000115.png', '000121.png', '000116.png', '000117.png', '000120.png', '000118.png', '000122.png', '000119.png', '000123.png', '000124.png', '000128.png', '000129.png', '000130.png', '000125.png', '000131.png', '000126.png', '000132.png', '000127.png', '000133.png', '000134.png', '000135.png', '000136.png', '000141.png', '000137.png', '000142.png', '000139.png', '000140.png', '000138.png', '000143.png', '000144.png', '000146.png', '000145.png', '000150.png', '000147.png', '000151.png', '000148.png', '000152.png', '000149.png', '000153.png', '000154.png', '000155.png', '000156.png', '000160.png', '000157.png', '000161.png', '000158.png', '000162.png', '000159.png', '000163.png', '000164.png', '000165.png', '000166.png', '000167.png', '000170.png', '000168.png', '000171.png', '000169.png', '000172.png', '000173.png', '000174.png', '000175.png', '000177.png', '000176.png', '000178.png', '000180.png', '000179.png', '000181.png', '000183.png', '000188.png', '000184.png', '000187.png', '000182.png', '000189.png', '000185.png', '000186.png', '000190.png', '000191.png', '000192.png', '000196.png', '000193.png', '000195.png', '000194.png', '000197.png', '000198.png', '000199.png']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llopTwRwoaUb",
        "colab_type": "text"
      },
      "source": [
        "#### Optional transform images to array first, then for model training, this is impractical for big data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfSMgSSoslKP",
        "colab_type": "code",
        "outputId": "fa5b11ce-20d9-4917-f8d6-0d9d7dfc1dba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "\n",
        "x_train = []\n",
        "i = 0\n",
        "for img in os.listdir(\"/content/drive/My Drive/Deep Learning projects/car segmentation/training data part/episode_0000/CameraRGB/training_imgs\"):\n",
        "  img_path = os.path.join(\"/content/drive/My Drive/Deep Learning projects/car segmentation/training data part/episode_0000/CameraRGB/training_imgs\", img)\n",
        "  img_array = cv2.imread(img_path)\n",
        "  #print(img_array)\n",
        "  x_train.append(img_array/255)\n",
        "    \n",
        "#new_array = cv2.resize(img_array, (img_size, img_size))\n",
        "  \n",
        "\n",
        "x_train = np.asarray(x_train)\n",
        "x_train.shape\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-f09e2e2d8fb4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/My Drive/Deep Learning projects/car segmentation/training data part/episode_0000/CameraRGB/training_imgs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mimg_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/My Drive/Deep Learning projects/car segmentation/training data part/episode_0000/CameraRGB/training_imgs\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0mimg_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   \u001b[0;31m#print(img_array)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_array\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNasgv_nsqtB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train = []\n",
        "i = 0\n",
        "for img in os.listdir(\"/content/drive/My Drive/Deep Learning projects/car segmentation/training data part/episode_0000/CameraSeg/training_labels\"):\n",
        "  img_path = os.path.join(\"/content/drive/My Drive/Deep Learning projects/car segmentation/training data part/episode_0000/CameraSeg/labels\", img)\n",
        "  img_array = cv2.imread(img_path)\n",
        "  y_train.append(img_array[:, :, 0:1])\n",
        "y_train = np.asarray(y_train)\n",
        "y_train.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27NH3hFdggT5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# remove the images not correct\n",
        "# !rm \"/content/drive/My Drive/Deep Learning projects/car segmentation/training data part/episode_0000/CameraSeg/._000020.png\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKrE-p-esluM",
        "colab_type": "text"
      },
      "source": [
        "## Step 3: Prepare for the U-Net Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4wUAbcIO69e",
        "colab_type": "text"
      },
      "source": [
        "A successful and popular model for object detection if the UNet architecture. It consists of a contracting path and an expansive path.\n",
        "\n",
        "The contracting path follows the typical architecture of a convolutional network. It consists of the repeated application of two 3 * 3 convolutions, each followed by a batchnormalization layer and a ReLU activation and dropout and a 2 * 2 max pooling operation with stride 2 for downsampling. At each downsampling step we double the number of feature channnels. THe purpose of this contracting path is to capture the context of the input image in order to be able to do segmentation.\n",
        "\n",
        "Every step in the expansive path consists of an upsampling of the feature map followed by a 2 * 2 convolution (\"up-convolution\") that halves the number of feature channels, a concatenation with the correspondingly feature map from the contracting path, and two 3 * 3 convolutions, each followed by batchnorm, dropout and a ReLU. The purpose of this expanding path is to enable precise localization combined with contextual information from the contracting path.\n",
        "\n",
        "At the final layer a 1 * 1 convolution is used to map each 16 - component feature vector to the desired number of classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFwtaN3VyhfR",
        "colab_type": "text"
      },
      "source": [
        "#### 3.1 Model creation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxTwKeIDSl3b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def conv2d_block(input_tensor, n_filters, kernel_size=3, batchnorm=True):\n",
        "    # first layer\n",
        "    x = Conv2D(filters=n_filters, kernel_size=(kernel_size, kernel_size), kernel_initializer=\"he_normal\",\n",
        "               padding=\"same\")(input_tensor)\n",
        "    if batchnorm:\n",
        "        x = BatchNormalization()(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    # second layer\n",
        "    x = Conv2D(filters=n_filters, kernel_size=(kernel_size, kernel_size), kernel_initializer=\"he_normal\",\n",
        "               padding=\"same\")(x)\n",
        "    if batchnorm:\n",
        "        x = BatchNormalization()(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-76-h9l1QBAq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_unet(input_img, n_filters=16, dropout=0.5, batchnorm=True):\n",
        "    # contracting path\n",
        "    c1 = conv2d_block(input_img, n_filters=n_filters*1, kernel_size=3, batchnorm=batchnorm)\n",
        "    p1 = MaxPooling2D((2, 2)) (c1)\n",
        "    p1 = Dropout(dropout*0.5)(p1)\n",
        "\n",
        "    c2 = conv2d_block(p1, n_filters=n_filters*2, kernel_size=3, batchnorm=batchnorm)\n",
        "    p2 = MaxPooling2D((2, 2)) (c2)\n",
        "    p2 = Dropout(dropout)(p2)\n",
        "\n",
        "    c3 = conv2d_block(p2, n_filters=n_filters*4, kernel_size=3, batchnorm=batchnorm)\n",
        "    p3 = MaxPooling2D((2, 2)) (c3)\n",
        "    p3 = Dropout(dropout)(p3)\n",
        "\n",
        "    c4 = conv2d_block(p3, n_filters=n_filters*8, kernel_size=3, batchnorm=batchnorm)\n",
        "    p4 = MaxPooling2D(pool_size=(2, 2)) (c4)\n",
        "    p4 = Dropout(dropout)(p4)\n",
        "    \n",
        "    c5 = conv2d_block(p4, n_filters=n_filters*16, kernel_size=3, batchnorm=batchnorm)\n",
        "    \n",
        "    # expansive path\n",
        "    u6 = Conv2DTranspose(n_filters*8, (3, 3), strides=(2, 2), padding='same') (c5)\n",
        "    u6 = concatenate([u6, c4])\n",
        "    u6 = Dropout(dropout)(u6)\n",
        "    c6 = conv2d_block(u6, n_filters=n_filters*8, kernel_size=3, batchnorm=batchnorm)\n",
        "\n",
        "    u7 = Conv2DTranspose(n_filters*4, (3, 3), strides=(2, 2), padding='same') (c6)\n",
        "    u7 = concatenate([u7, c3])\n",
        "    u7 = Dropout(dropout)(u7)\n",
        "    c7 = conv2d_block(u7, n_filters=n_filters*4, kernel_size=3, batchnorm=batchnorm)\n",
        "\n",
        "    u8 = Conv2DTranspose(n_filters*2, (3, 3), strides=(2, 2), padding='same') (c7)\n",
        "    u8 = concatenate([u8, c2])\n",
        "    u8 = Dropout(dropout)(u8)\n",
        "    c8 = conv2d_block(u8, n_filters=n_filters*2, kernel_size=3, batchnorm=batchnorm)\n",
        "\n",
        "    u9 = Conv2DTranspose(n_filters*1, (3, 3), strides=(2, 2), padding='same') (c8)\n",
        "    u9 = concatenate([u9, c1], axis=3)\n",
        "    u9 = Dropout(dropout)(u9)\n",
        "    c9 = conv2d_block(u9, n_filters=n_filters*1, kernel_size=3, batchnorm=batchnorm)\n",
        "    \n",
        "    outputs = Conv2D(1, (1, 1), activation='sigmoid') (c9)\n",
        "    model = Model(inputs=[input_img], outputs=[outputs])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBGXdaFDYssD",
        "colab_type": "code",
        "outputId": "17bed10c-57b9-4b3c-a3c5-8aca42ce3ac8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "im_height = 256\n",
        "im_width = 800\n",
        "\n",
        "input_img = Input((im_height, im_width, 3), name='img')\n",
        "model = get_unet(input_img, n_filters=16, dropout=0.05, batchnorm=True)\n",
        "\n",
        "model.compile(optimizer=Adam(), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "model.summary()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0805 22:13:11.217647 139875181086592 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0805 22:13:11.285825 139875181086592 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0805 22:13:11.288162 139875181086592 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4185: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
            "\n",
            "W0805 22:13:11.339312 139875181086592 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "W0805 22:13:11.340676 139875181086592 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "W0805 22:13:14.745691 139875181086592 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "W0805 22:13:15.056562 139875181086592 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "W0805 22:13:15.068687 139875181086592 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0805 22:13:17.088635 139875181086592 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0805 22:13:17.104833 139875181086592 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "img (InputLayer)                (None, 256, 800, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 256, 800, 16) 448         img[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 256, 800, 16) 64          conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 256, 800, 16) 0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 256, 800, 16) 2320        activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 256, 800, 16) 64          conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 256, 800, 16) 0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 128, 400, 16) 0           activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 128, 400, 16) 0           max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 128, 400, 32) 4640        dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 128, 400, 32) 128         conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 128, 400, 32) 0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 128, 400, 32) 9248        activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 128, 400, 32) 128         conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 128, 400, 32) 0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 64, 200, 32)  0           activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 64, 200, 32)  0           max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 64, 200, 64)  18496       dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 64, 200, 64)  256         conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 64, 200, 64)  0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 64, 200, 64)  36928       activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 64, 200, 64)  256         conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 64, 200, 64)  0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, 32, 100, 64)  0           activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 32, 100, 64)  0           max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 32, 100, 128) 73856       dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 32, 100, 128) 512         conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 32, 100, 128) 0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 32, 100, 128) 147584      activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 32, 100, 128) 512         conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 32, 100, 128) 0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2D)  (None, 16, 50, 128)  0           activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 16, 50, 128)  0           max_pooling2d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 16, 50, 256)  295168      dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 16, 50, 256)  1024        conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 16, 50, 256)  0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 16, 50, 256)  590080      activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 16, 50, 256)  1024        conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 16, 50, 256)  0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_1 (Conv2DTrans (None, 32, 100, 128) 295040      activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 32, 100, 256) 0           conv2d_transpose_1[0][0]         \n",
            "                                                                 activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_5 (Dropout)             (None, 32, 100, 256) 0           concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 32, 100, 128) 295040      dropout_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 32, 100, 128) 512         conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 32, 100, 128) 0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 32, 100, 128) 147584      activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 32, 100, 128) 512         conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 32, 100, 128) 0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_2 (Conv2DTrans (None, 64, 200, 64)  73792       activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 64, 200, 128) 0           conv2d_transpose_2[0][0]         \n",
            "                                                                 activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_6 (Dropout)             (None, 64, 200, 128) 0           concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 64, 200, 64)  73792       dropout_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 64, 200, 64)  256         conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 64, 200, 64)  0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 64, 200, 64)  36928       activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 64, 200, 64)  256         conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 64, 200, 64)  0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_3 (Conv2DTrans (None, 128, 400, 32) 18464       activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 128, 400, 64) 0           conv2d_transpose_3[0][0]         \n",
            "                                                                 activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_7 (Dropout)             (None, 128, 400, 64) 0           concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 128, 400, 32) 18464       dropout_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 128, 400, 32) 128         conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 128, 400, 32) 0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 128, 400, 32) 9248        activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 128, 400, 32) 128         conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 128, 400, 32) 0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_4 (Conv2DTrans (None, 256, 800, 16) 4624        activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_4 (Concatenate)     (None, 256, 800, 32) 0           conv2d_transpose_4[0][0]         \n",
            "                                                                 activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_8 (Dropout)             (None, 256, 800, 32) 0           concatenate_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 256, 800, 16) 4624        dropout_8[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 256, 800, 16) 64          conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 256, 800, 16) 0           batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 256, 800, 16) 2320        activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 256, 800, 16) 64          conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 256, 800, 16) 0           batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 256, 800, 1)  17          activation_18[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 2,164,593\n",
            "Trainable params: 2,161,649\n",
            "Non-trainable params: 2,944\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZegjJNacyr9u",
        "colab_type": "text"
      },
      "source": [
        "#### 3.2 Model visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFZQDpEDkXvB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils import plot_model\n",
        "plot_model(model, to_file='model.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7q9QONNByvBQ",
        "colab_type": "text"
      },
      "source": [
        "#### 3.3 Model fitting/training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTqSLjg4Y_ik",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "callbacks = [\n",
        "    EarlyStopping(patience=10, verbose=1),\n",
        "    ReduceLROnPlateau(factor=0.1, patience=30, min_lr=0.00001, verbose=1),\n",
        "    ModelCheckpoint('model-tgs-salt.h5', verbose=1, save_best_only=True, save_weights_only=True)\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbFSD3OnZEfa",
        "colab_type": "code",
        "outputId": "e8b146a5-8c7d-46eb-c4f3-486031fbb695",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        }
      },
      "source": [
        "steps_per_epoch = 150/16\n",
        "validation_steps = 30/16\n",
        "\n",
        "history = model.fit_generator(train_gen, \n",
        "                              epochs = 100,\n",
        "                              steps_per_epoch = steps_per_epoch, # number of training_images / batch_size\n",
        "                              validation_data = val_gen,\n",
        "                              validation_steps = validation_steps)\n",
        "\n",
        "model.save('car_detection_Model.h5')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "9/9 [===========================>..] - ETA: 1s - loss: 0.9759 - acc: 0.0608"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-b685314b3a9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# number of training_images / batch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                               \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_gen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                               validation_steps = validation_steps)\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'car_detection_Model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    232\u001b[0m                             \u001b[0mval_enqueuer_gen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m                             \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                             workers=0)\n\u001b[0m\u001b[1;32m    235\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                         \u001b[0;31m# No need for try/except because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate_generator\u001b[0;34m(self, generator, steps, max_queue_size, workers, use_multiprocessing, verbose)\u001b[0m\n\u001b[1;32m   1470\u001b[0m             \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1471\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m             verbose=verbose)\n\u001b[0m\u001b[1;32m   1473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mevaluate_generator\u001b[0;34m(model, generator, steps, max_queue_size, workers, use_multiprocessing, verbose)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m             \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m                 raise ValueError('Output of generator should be a tuple '\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    707\u001b[0m                     \u001b[0;34m\"`use_multiprocessing=False, workers > 1`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m                     \"For more information see issue #1638.\")\n\u001b[0;32m--> 709\u001b[0;31m             \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/six.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    691\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    694\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    683\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m                 \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    642\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mworker\u001b[0;34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mjob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mwrap_exception\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_helper_reraises_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mnext_sample\u001b[0;34m(uid)\u001b[0m\n\u001b[1;32m    624\u001b[0m         \u001b[0mThe\u001b[0m \u001b[0mnext\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0mof\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0muid\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m     \"\"\"\n\u001b[0;32m--> 626\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_SHARED_SEQUENCES\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-3309c37315de>\u001b[0m in \u001b[0;36mdata_gen\u001b[0;34m(img_folder, mask_folder, batch_size)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# initially from 0 to 16, c = 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m       \u001b[0mtrain_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_folder\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m       \u001b[0;31m#train_img = cv2.resize(train_img, (512, 512)) # Read an image from folder and resize the image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogQEjF3ly7s-",
        "colab_type": "text"
      },
      "source": [
        "## Step 4: Model accuracy & loss analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHd4hjh3ZIgM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(8, 8))\n",
        "plt.title(\"Learning curve\")\n",
        "plt.plot(history.history[\"loss\"], label=\"loss\")\n",
        "plt.plot(history.history[\"val_loss\"], label=\"val_loss\")\n",
        "plt.plot( np.argmin(history.history[\"val_loss\"]), np.min(history.history[\"val_loss\"]), marker=\"x\", color=\"r\", label=\"best model\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"log_loss\")\n",
        "plt.legend();"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrLr40FPIUr9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(8, 8))\n",
        "plt.title(\"Learning curve\")\n",
        "plt.plot(history.history[\"acc\"], label=\"acc\")\n",
        "plt.plot(history.history[\"val_acc\"], label=\"val_acc\")\n",
        "#plt.plot( np.argmin(history.history[\"val_acc\"]), np.min(history.history[\"val_acc\"]), marker=\"x\", color=\"r\", label=\"best model\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"log_acc\")\n",
        "plt.legend();"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lO66hg7GZN0H",
        "colab_type": "text"
      },
      "source": [
        "## Step 5: Inference With the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Unt8dwvMZQg6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load best model\n",
        "model.load_weights('model-tgs-salt.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHEAJUODZS8_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Evaluate on validation set (this must be equals to the best log_loss)\n",
        "model.evaluate(X_valid, y_valid, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAE1pF4qZXcN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Predict on train, val and test\n",
        "preds_train = model.predict(X_train, verbose=1)\n",
        "preds_val = model.predict(X_valid, verbose=1)\n",
        "\n",
        "# Threshold predictions\n",
        "preds_train_t = (preds_train > 0.5).astype(np.uint8)\n",
        "preds_val_t = (preds_val > 0.5).astype(np.uint8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRBCnFgtZZ4Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_sample(X, y, preds, binary_preds, ix=None):\n",
        "    if ix is None:\n",
        "        ix = random.randint(0, len(X))\n",
        "\n",
        "    has_mask = y[ix].max() > 0\n",
        "\n",
        "    fig, ax = plt.subplots(1, 4, figsize=(20, 10))\n",
        "    ax[0].imshow(X[ix, ..., 0], cmap='seismic')\n",
        "    if has_mask:\n",
        "        ax[0].contour(y[ix].squeeze(), colors='k', levels=[0.5])\n",
        "    ax[0].set_title('Seismic')\n",
        "\n",
        "    ax[1].imshow(y[ix].squeeze())\n",
        "    ax[1].set_title('Salt')\n",
        "\n",
        "    ax[2].imshow(preds[ix].squeeze(), vmin=0, vmax=1)\n",
        "    if has_mask:\n",
        "        ax[2].contour(y[ix].squeeze(), colors='k', levels=[0.5])\n",
        "    ax[2].set_title('Salt Predicted')\n",
        "    \n",
        "    ax[3].imshow(binary_preds[ix].squeeze(), vmin=0, vmax=1)\n",
        "    if has_mask:\n",
        "        ax[3].contour(y[ix].squeeze(), colors='k', levels=[0.5])\n",
        "    ax[3].set_title('Salt Predicted binary');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzHWhGllvcjo",
        "colab_type": "text"
      },
      "source": [
        "## Useful reference\n",
        "URL_1 = \"https://towardsdatascience.com/faster-r-cnn-object-detection-implemented-by-keras-for-custom-data-from-googles-open-images-125f62b9141a\"\n",
        "\n",
        "URL_2 = \"https://tryolabs.com/blog/2018/01/18/faster-r-cnn-down-the-rabbit-hole-of-modern-object-detection/\"\n",
        "\n",
        "URL_3 = \"https://tryolabs.com/blog/2017/08/30/object-detection-an-overview-in-the-age-of-deep-learning/\" **(include datasets)**\n",
        "\n",
        "URL_4 = \"https://tryolabs.com/blog/2018/01/18/faster-r-cnn-down-the-rabbit-hole-of-modern-object-detection/\"\n",
        "\n",
        "URL_5 = \"https://towardsdatascience.com/deep-learning-for-object-detection-a-comprehensive-review-73930816d8d9\" **(Deep learning for object Detection)**\n",
        "\n",
        "URL_6 = \"https://towardsdatascience.com/deep-learning-for-object-detection-a-comprehensive-review-73930816d8d9\"\n",
        "\n",
        "URL_7 = \"https://www.depends-on-the-definition.com/unet-keras-segmenting-images/\" **(unet model)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k23oBXhMvfGC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}