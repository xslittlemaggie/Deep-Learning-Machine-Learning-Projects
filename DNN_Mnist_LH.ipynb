{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DNN_Mnist_LH.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xslittlemaggie/Deep-Learning-Machine-Learning-Projects/blob/master/DNN_Mnist_LH.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IdKbNDks5sy",
        "colab_type": "text"
      },
      "source": [
        "## Step 1: Import librarious"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cbi4948Ys3bY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7947cf63-fed0-4e2b-a65f-621f5f8e89ea"
      },
      "source": [
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Activation\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten\n",
        "from keras.optimizers import SGD, Adam\n",
        "from keras.utils import np_utils\n",
        "from keras.datasets import mnist"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9604MxXtDt4",
        "colab_type": "text"
      },
      "source": [
        "## Step 2: Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmTJ3YZWqO2j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data():\n",
        "  (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "  number = 10000\n",
        "  x_train = x_train[0:number]\n",
        "  y_train = y_train[0:number]\n",
        "  x_train = x_train.reshape(number, 28 * 28)\n",
        "  x_test = x_test.reshape(x_test.shape[0], 28 * 28)\n",
        "  x_train = x_train.astype('float32')\n",
        "  x_test = x_test.astype('float32')\n",
        "  # convert class vectors to binary class matrices\n",
        "  y_train = np_utils.to_categorical(y_train, 10)\n",
        "  y_test = np_utils.to_categorical(y_test, 10)\n",
        "  \n",
        "  x_train = x_train\n",
        "  x_test = x_test\n",
        "  \n",
        "  x_train = x_train/255   # normalize the input, if not normalized, the accuracy decreases significantly\n",
        "  x_test = x_test/255\n",
        "  #x_test = np.random.normal(x_test)  # add random noise to testing data, this would decrease the test accuracy\n",
        "  return (x_train, y_train), (x_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBYJB-FvqVvH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "20e387ee-9f7a-405e-ceec-077fe7685640"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = load_data()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUSQTE4DtJXw",
        "colab_type": "text"
      },
      "source": [
        "## Step 3: Get familiar with the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjjaYXfzqrx7",
        "colab_type": "code",
        "outputId": "a4b7b55e-3f5f-4f3a-e214-0a5d07b19729",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "print(\"x_train shape:\", x_train.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"x_test shape:\", x_test.shape)\n",
        "print(\"y_test shape:\", y_test.shape)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (10000, 784)\n",
            "y_train shape: (10000, 10)\n",
            "x_test shape: (10000, 784)\n",
            "y_test shape: (10000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lWNp13nqyeI",
        "colab_type": "code",
        "outputId": "7f1c3404-e4ac-4d53-e2a7-c25fe0d1952c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(y_train[0]) # the first value is 5"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e40ewWHUuN57",
        "colab_type": "text"
      },
      "source": [
        "## Step 4: Create the DNN model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHp3YTBgzV1x",
        "colab_type": "text"
      },
      "source": [
        "Models:\n",
        "      **1). Dense -> fully connected NN**\n",
        "      **2). MaxPooling**\n",
        "      **3). Conv2D -> CNN**\n",
        "      **4). Flatten**\n",
        "\n",
        "\n",
        "1. Activation functions: \n",
        "      **1). relu** (more efficient)\n",
        "      **2). sigmoid**\n",
        "      **3). tanh**\n",
        "      **4). softmax** (usually the last layer)\n",
        "      \n",
        "2. Loss functions: \n",
        "      **1). mse** (not good for classification)\n",
        "      **2). categorical_crossentropy**\n",
        "      \n",
        "3. Optimizers: \n",
        "      **1). SGD(lr = 0.01)**\n",
        "      **2). Adam**\n",
        "      \n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wm0gF8PWsvy9",
        "colab_type": "code",
        "outputId": "1c265ad5-05b9-477b-c886-e6eb2b5ac9ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(input_dim = 28 * 28, units = 512, activation = 'relu'))  # only need to add the input_dim at the first layer)\n",
        "#model.add(Dropout(0.2))  # if there is overfitting, use dropout, add dropout at every hidden\n",
        "\n",
        "model.add(Dense(units = 512, activation = 'relu'))\n",
        "#model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(units = 512, activation = 'relu'))\n",
        "#model.add(Dropout(0.2))\n",
        "\n",
        "#for i in range(10):  # add more layers, not work\n",
        "#  model.add(Dense(units = 512, activation = 'relu'))   \n",
        "\n",
        "\n",
        "model.add(Dense(units = 10, activation = 'softmax')) # the units (neurons) of the last layer need to be the # of classes\n",
        "\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer = \"adam\", metrics = ['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train, batch_size = 100, epochs = 30)\n",
        "\n",
        "result = model.evaluate(x_train, y_train, batch_size = 10000)\n",
        "print(\"Train Acc:\", result[1])\n",
        "print()\n",
        "\n",
        "result = model.evaluate(x_test, y_test, batch_size = 10000)\n",
        "print(\"Test Acc:\", result[1])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0716 01:23:56.845124 140663810242432 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0716 01:23:56.887720 140663810242432 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0716 01:23:56.895973 140663810242432 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0716 01:23:56.957959 140663810242432 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0716 01:23:56.990153 140663810242432 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "W0716 01:23:57.124795 140663810242432 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0716 01:23:57.194418 140663810242432 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "10000/10000 [==============================] - 5s 458us/step - loss: 0.4667 - acc: 0.8640\n",
            "Epoch 2/30\n",
            "10000/10000 [==============================] - 1s 68us/step - loss: 0.1585 - acc: 0.9527\n",
            "Epoch 3/30\n",
            "10000/10000 [==============================] - 1s 68us/step - loss: 0.0923 - acc: 0.9710\n",
            "Epoch 4/30\n",
            "10000/10000 [==============================] - 1s 71us/step - loss: 0.0503 - acc: 0.9848\n",
            "Epoch 5/30\n",
            "10000/10000 [==============================] - 1s 72us/step - loss: 0.0397 - acc: 0.9876\n",
            "Epoch 6/30\n",
            "10000/10000 [==============================] - 1s 73us/step - loss: 0.0277 - acc: 0.9910\n",
            "Epoch 7/30\n",
            "10000/10000 [==============================] - 1s 74us/step - loss: 0.0386 - acc: 0.9874\n",
            "Epoch 8/30\n",
            "10000/10000 [==============================] - 1s 73us/step - loss: 0.0198 - acc: 0.9932\n",
            "Epoch 9/30\n",
            "10000/10000 [==============================] - 1s 74us/step - loss: 0.0088 - acc: 0.9975\n",
            "Epoch 10/30\n",
            "10000/10000 [==============================] - 1s 71us/step - loss: 0.0131 - acc: 0.9954\n",
            "Epoch 11/30\n",
            "10000/10000 [==============================] - 1s 72us/step - loss: 0.0174 - acc: 0.9948\n",
            "Epoch 12/30\n",
            "10000/10000 [==============================] - 1s 71us/step - loss: 0.0163 - acc: 0.9950\n",
            "Epoch 13/30\n",
            "10000/10000 [==============================] - 1s 70us/step - loss: 0.0312 - acc: 0.9897\n",
            "Epoch 14/30\n",
            "10000/10000 [==============================] - 1s 69us/step - loss: 0.0270 - acc: 0.9913\n",
            "Epoch 15/30\n",
            "10000/10000 [==============================] - 1s 73us/step - loss: 0.0211 - acc: 0.9928\n",
            "Epoch 16/30\n",
            "10000/10000 [==============================] - 1s 70us/step - loss: 0.0137 - acc: 0.9953\n",
            "Epoch 17/30\n",
            "10000/10000 [==============================] - 1s 71us/step - loss: 0.0027 - acc: 0.9994\n",
            "Epoch 18/30\n",
            "10000/10000 [==============================] - 1s 70us/step - loss: 0.0063 - acc: 0.9983\n",
            "Epoch 19/30\n",
            "10000/10000 [==============================] - 1s 70us/step - loss: 0.0180 - acc: 0.9948\n",
            "Epoch 20/30\n",
            "10000/10000 [==============================] - 1s 69us/step - loss: 0.0071 - acc: 0.9980\n",
            "Epoch 21/30\n",
            "10000/10000 [==============================] - 1s 70us/step - loss: 0.0021 - acc: 0.9993\n",
            "Epoch 22/30\n",
            "10000/10000 [==============================] - 1s 71us/step - loss: 9.8466e-04 - acc: 0.9999\n",
            "Epoch 23/30\n",
            "10000/10000 [==============================] - 1s 70us/step - loss: 0.0029 - acc: 0.9992\n",
            "Epoch 24/30\n",
            "10000/10000 [==============================] - 1s 71us/step - loss: 0.0259 - acc: 0.9916\n",
            "Epoch 25/30\n",
            "10000/10000 [==============================] - 1s 71us/step - loss: 0.0290 - acc: 0.9913\n",
            "Epoch 26/30\n",
            "10000/10000 [==============================] - 1s 69us/step - loss: 0.0170 - acc: 0.9954\n",
            "Epoch 27/30\n",
            "10000/10000 [==============================] - 1s 70us/step - loss: 0.0061 - acc: 0.9978\n",
            "Epoch 28/30\n",
            "10000/10000 [==============================] - 1s 69us/step - loss: 0.0060 - acc: 0.9978\n",
            "Epoch 29/30\n",
            "10000/10000 [==============================] - 1s 69us/step - loss: 0.0010 - acc: 0.9997\n",
            "Epoch 30/30\n",
            "10000/10000 [==============================] - 1s 69us/step - loss: 2.4099e-04 - acc: 1.0000\n",
            "10000/10000 [==============================] - 0s 9us/step\n",
            "Train Acc: 1.0\n",
            "\n",
            "10000/10000 [==============================] - 0s 5us/step\n",
            "Test Acc: 0.9678999781608582\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWkq9xZNOdM-",
        "colab_type": "text"
      },
      "source": [
        "Result analysis:\n",
        "\n",
        "**Train** acc: 0.98\n",
        "\n",
        "**Test** acc: 0.96\n",
        "\n",
        "It looks good!\n",
        "\n"
      ]
    }
  ]
}