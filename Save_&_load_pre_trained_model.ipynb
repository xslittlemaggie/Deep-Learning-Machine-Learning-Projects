{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Save & load pre-trained model.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xslittlemaggie/Deep-Learning-Machine-Learning-Projects/blob/master/Save_%26_load_pre_trained_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVEixmhu9pKj",
        "colab_type": "text"
      },
      "source": [
        "## Step 0: Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40Gvuauo9a4h",
        "colab_type": "code",
        "outputId": "b85614d6-50a4-4b00-c53c-ba6ba5b4c267",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.optimizers import SGD, Adam, Adadelta\n",
        "from keras import backend as K\n",
        "from keras.utils import np_utils\n",
        "import numpy as np\n",
        "import random\n",
        "% matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Confusion matrix result\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# visualizating of confusion matrix\n",
        "import seaborn as sn\n",
        "import pandas as pd\n",
        "\n",
        "# upload new images\n",
        "import urllib.request\n",
        "import os\n",
        "import cv2"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qq0zLf1whlZC",
        "colab_type": "text"
      },
      "source": [
        "## Step 1: Load cifar10 data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bp-xg9Jc9kNu",
        "colab_type": "code",
        "outputId": "d643bbca-ea2c-4b19-add4-3eeedbaae072",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 4s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KeMHF6uDYAER",
        "colab_type": "code",
        "outputId": "3e5f54c7-6166-4dc8-dbbc-02d28ce677a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "print(\"x_train shape:\", x_train.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "\n",
        "print(\"x_test shape:\", x_test.shape)\n",
        "print(\"y_test shape:\", y_test.shape)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (50000, 32, 32, 3)\n",
            "y_train shape: (50000, 10)\n",
            "x_test shape: (10000, 32, 32, 3)\n",
            "y_test shape: (10000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WopIXXUMaYos",
        "colab_type": "text"
      },
      "source": [
        "## Step 2: Data preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ievkp1x7CUxx",
        "colab_type": "text"
      },
      "source": [
        "The pixel values are in the range of 0 to 255 for each of the red, green and blue channels.\n",
        "\n",
        "It is good practice to work with normalized data.\n",
        "\n",
        "We can easily normalize to range 0 to 1 by dividing each value by the maximum observation which is 255."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dcngzc8sVG_5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if K.image_data_format() == \"channels_first\":\n",
        "  x_train = x_train.reshape(x_train.shape[0], 3, 32, 32)\n",
        "  x_test = x_test.reshape(x_test.shape[0], 3, 32, 32)\n",
        "  input_shape = (3, 32, 32)\n",
        "  \n",
        "else:\n",
        "  x_train = x_train.reshape(x_train.shape[0], 32, 32, 3)\n",
        "  x_test = x_test.reshape(x_test.shape[0], 32, 32, 3)\n",
        "  input_shape = (32, 32, 3)\n",
        "  \n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 225.\n",
        "x_test /= 225.\n",
        "\n",
        "# convert class vectors to binary class matrics\n",
        "y_train = np_utils.to_categorical(y_train, 10)\n",
        "y_test = np_utils.to_categorical(y_test, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEH-oOScrZMC",
        "colab_type": "text"
      },
      "source": [
        "## Load and use weights from a checkpoint ('weights.best.hdf5' trained before)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZkRwiInvV-g",
        "colab_type": "code",
        "outputId": "3299c0d7-1a09-4d9f-b38e-d3cc82298d36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "num_classes = 10\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), padding='same',\n",
        "                 input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Conv2D(32, (3, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25)) # Add dropout layer here\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25)) # Add dropout layer here\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5)) # Add dropout layer here\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "# load pre-trained weights \n",
        "model.load_weights('weights.best.hdf5')\n",
        "model.compile(loss = 'categorical_crossentropy', # better loss function for neural networks\n",
        "            optimizer = 'adam', # Adam optimizer with 1.0e-4 learning rate\n",
        "            metrics = ['accuracy'])\n",
        "print('Created model and loaded weights from file')  \n",
        "\n",
        "# load pima indians dataset\n",
        "scores = model.evaluate(x_test, y_test, verbose = 0)\n",
        "print('Test Acc: {}'.format(scores[1]))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0802 16:00:04.430826 140161707186048 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Created model and loaded weights from file\n",
            "Test Acc: 0.8239\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0q0HsfYtSKS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}