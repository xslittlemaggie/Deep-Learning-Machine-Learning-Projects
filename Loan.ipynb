{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Loan",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xslittlemaggie/Deep-Learning-Machine-Learning-Projects/blob/master/Loan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "159KG_EtG9LA",
        "colab_type": "text"
      },
      "source": [
        "<h1><center> LendingClub Loan Data Analysis & Model Building Report </center></h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Akc2wpG0MVBe",
        "colab_type": "text"
      },
      "source": [
        "<h1>Table of contents</h1>\n",
        "\n",
        "<div class=\"alert alert-block alert-info\" style=\"margin-top: 24px\">\n",
        "    <ol>\n",
        "        <li><a href=\"#purpose of the study\">Project Understanding</a></li>\n",
        "        <li><a href=\"#loading_data\">Data Loading</a></li>\n",
        "        <li><a href=\"#features exploratory\">Data Exploratory</a></li>\n",
        "        <li><a href=\"#data preprocessing\">Data Preprocessing</a></li>\n",
        "        <li><a href=\"#features engineering\">Features Engineering</a></li>\n",
        "        <li><a href=\"#model building\">Model Buildings</a></li>\n",
        "        <li><a href=\"#conclusion\">Conclusion</a></li>\n",
        "    </ol>\n",
        "</div>\n",
        "<br>\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byy2FDTico1k",
        "colab_type": "text"
      },
      "source": [
        "# 0: Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XMWP5dFt7OP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import mode\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzfMtx8OgePX",
        "colab_type": "text"
      },
      "source": [
        "# 1. Project Understanding\n",
        "\n",
        "**1. The Purpose：**\n",
        "\n",
        "The purpose of this project is to analyze the features related to the loan risks and create models (high accuracy, auc scores) to evaluate the risks in loan. \n",
        "\n",
        "**2. The Data:**\n",
        "\n",
        "The LendingClub dataset loaded from Kaggle competition will be used for this project. \n",
        "\n",
        "The raw dataset includes 42538 samples and 145 features, which is large enough for data analysis. \n",
        "\n",
        "**3. The models:**\n",
        "\n",
        "(1). Logistic Regression: Easy understanding\n",
        "\n",
        "(2). Random Forest: Complex, could be more accurate, but less explainable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ua-1kaIBgLXG",
        "colab_type": "text"
      },
      "source": [
        "# 2. Load dataset from Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4g0twFhd0NG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "os.environ['KAGGLE_USERNAME'] = \"liulihuang\" # username from the json file \n",
        "os.environ['KAGGLE_KEY'] = \"7adfc6c4e6c5eec087031fbb7397aee5\" # key from the json file (This key is incorrect) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDIPidpRfFmI",
        "colab_type": "code",
        "outputId": "18bb3e27-3811-48af-cb9b-7cfb1e95891f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install -q kaggle\n",
        "!kaggle datasets list -s lending-club-loan-data  # It will list the 20 datasets including \"lending-club-loan-data\" from kaggle\n",
        "!kaggle datasets download -d balaji1994/lending-club-loan-data -p /content/\n",
        "!unzip -q /content/lending-club-loan-data.zip -d /content/lending-club-loan-data/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ref                                  title                        size  lastUpdated          downloadCount  \n",
            "-----------------------------------  --------------------------  -----  -------------------  -------------  \n",
            "wendykan/lending-club-loan-data      Lending Club Loan Data      702MB  2019-03-18 18:43:12          57935  \n",
            "wordsforthewise/lending-club         All Lending Club loan data  618MB  2019-04-10 18:03:34           5739  \n",
            "skihikingkevin/online-p2p-lending    Online P2P Lending           77MB  2018-08-30 20:32:43            711  \n",
            "mrferozi/loan-data-for-dummy-bank    Loan Data for Dummy Bank     28MB  2018-08-04 12:45:51           1518  \n",
            "saurabh13nov/lending-club-loan-data  Lending Club Loan Data      105MB  2017-08-08 20:43:56            107  \n",
            "balaji1994/lending-club-loan-data    Lending Club Loan Data       35MB  2018-11-20 06:49:37             47  \n",
            "savasy/loan-data-sampled             Loan data sampled             1MB  2017-11-21 08:28:34             39  \n",
            "lending-club-loan-data.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atM2scKgfogn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loans = pd.read_csv(\"/content/lending-club-loan-data/LoanStats3a.csv\", skiprows = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQniDuzkiyti",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# the dictionary to check the details of the features\n",
        "#dic = pd.read_excel(\"/content/lending-club-loan-data/LCDataDictionary (1).xlsx\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPILB_dDg4zU",
        "colab_type": "text"
      },
      "source": [
        "# 3. Data Exploratory and some easy data preprocessing\n",
        "\n",
        "\n",
        "Get familiar with the data before data preprocessing, feature engineering, and model building. \n",
        "Drop the features which are obviously unrelated to the target, with all missing values, or with the same values (small variance)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SecfrGYWhFft",
        "colab_type": "text"
      },
      "source": [
        "## 3.1 Get familar with the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iw0eP6JgQlHG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loans.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdyzMG7ffxwh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# read the first five samples\n",
        "# loans.head()  # commented to save space"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJtH1ltJgS9A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# descriptive about the features\n",
        "# loans.describe().T  # commented to save space"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmuB5lV3QzvQ",
        "colab_type": "text"
      },
      "source": [
        "At the first glance of the data, there are a lot of features with all missing values. \n",
        "\n",
        "It it very obvious that these features will be removed from the dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qnETM8lzIk9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# select features which >99% of the data is missing\n",
        "all_missing_features = [feature for feature in loans.columns if ((loans[feature].isnull().sum())/loans.shape[0] > 0.99)]\n",
        "# drop all missing value features\n",
        "loans.drop(all_missing_features, axis = 1, inplace = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8JVLRk2RUJX",
        "colab_type": "text"
      },
      "source": [
        "After the first round, 88 features with all missing values are removed from the dataset.\n",
        "\n",
        "The dataset becomes more readable and managable. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWQtYZnABKzQ",
        "colab_type": "text"
      },
      "source": [
        "## 3.2 Go through each of the rest features roughly & prepare for the data engineering later\n",
        "1. **numerical features**, consider to drop the features very unbalanced\n",
        "2. **string, text features**, need recoding later \n",
        "3. **features with the same value**, such feature wond't contribute to the model, consider to drop these features\n",
        "4. **other features**, obviously not related to the target, or are highly related to other features, "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgsZwX9Z3oq5",
        "colab_type": "text"
      },
      "source": [
        "### 3.2.1 After deleting the features with all missing values, go through each of the rest feature, and cluster them into different class, numerical, categorical, text, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0e0_Ltm3ZuZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Number of features: {}, sample size: {}\".format(loans.shape[1], loans.shape[0]))\n",
        "all_features = loans.columns\n",
        "#print(all_features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pO0g3UoMx_DJ",
        "colab_type": "text"
      },
      "source": [
        "### Sample size : 42538\n",
        "\n",
        "**1. Numerical features**\n",
        "\n",
        "- \"loan_amnt\": \n",
        "- \"funded_amnt\":\n",
        "- \"funded_amnt_inv\": \n",
        "- \"installment\": \n",
        "- \"annual_inc\": \n",
        "- \"dti\":\n",
        "- \"delinq_2yrs\": \n",
        "- \"inq_last_6mths\": \n",
        "- \"mths_since_last_delinq\": \n",
        "- \"mths_since_last_record\": very unbalanced, 38887 (91.4%) missing values\n",
        "- \"open_acc\": \n",
        "- \"pub_rec\":\n",
        "- \"revol_bal\": \n",
        "- \"total_acc\": \n",
        "- \"total_pymnt\":\n",
        "- \"total_pymnt_inv\": \n",
        "- \"total_rec_prncp\": \n",
        "- \"total_rec_int\": \n",
        "- \"total_rec_late_fee\": \n",
        "- \"last_pymnt_amnt\": \n",
        "- \"pub_rec_bankruptcies\": very unbalanced, more than 39316 zeros(92.4%)\n",
        "\n",
        "\n",
        "**2. Categorical features: will be encoded during the feature enginerring step**\n",
        "- \"term\": e.g. 36 months\n",
        "- \"grade\": e.g. A\n",
        "- \"sub_grade\": e.g. B2 \n",
        "- \"emp_title\": e.g. US Army. This feature include 28862 different types of titles. \n",
        "- \"emp_length\": e.g. 10+ years\n",
        "- \"home_ownership\": e.g. RENT \n",
        "- \"verification_status\": e.g. Verified\n",
        "- \"purpose\": e.g. debt_consolidation\n",
        "- \"title\": e.g. Home Improvement, wedding, very simialr to purpose, but with more class \n",
        "- \"addr_state\": \n",
        "\n",
        "**3. num_features_in_str, need recoding**\n",
        "\n",
        "- \"int_rate\": e.g. 10%\n",
        "- \"revol_util\": e.g. 10%\n",
        "\n",
        "**4. date time features, e.g. could be encoded to get time duration**\n",
        "- \"issue_d\": e.g. Dec-2011\n",
        "- \"earliest_cr_line\": \n",
        "- \"last_pymnt_d\":\n",
        "- \"next_pymnt_d\":\n",
        "- \"last_credit_pull_d\":  \n",
        "\n",
        "**5. other features not related to the project or related to other features, consider drop**\n",
        "- \"zip_code\"\n",
        "- \"desc\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVeInzLPgKNM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# delete the features with 99% or more missing values or same values\n",
        "\n",
        "features_with_same_value = [\"next_pymnt_d\", \"pymnt_plan\", \"initial_list_status\", \"out_prncp\", \"out_prncp_inv\", \"collections_12_mths_ex_med\", \"policy_code\", \"application_type\", \"acc_now_delinq\",\n",
        "                            \"chargeoff_within_12_mths\", \"delinq_amnt\", \"tax_liens\", \"hardship_flag\", \"disbursement_method\", \"debt_settlement_flag\"]\n",
        "\n",
        "updated_features = [feature for feature in all_features if feature not in features_with_same_value]\n",
        "loans = loans[updated_features]\n",
        "#print(len(updated_features))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kc75Y_jHj3lL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# numerical features\n",
        "#[i for i in loans.columns[2:30] if loans[i].dtype == 'float']\n",
        "num_features = [\"loan_amnt\", \"funded_amnt\", \"funded_amnt_inv\", \"installment\", \"annual_inc\", \"dti\", \"delinq_2yrs\", \"inq_last_6mths\", \"mths_since_last_delinq\", \"mths_since_last_record\",\n",
        "               \"open_acc\", \"pub_rec\", \"revol_bal\", \"total_acc\", \"total_pymnt\", \"total_pymnt_inv\", \"total_rec_prncp\", \"total_rec_int\", \"total_rec_late_fee\", \"recoveries\",\n",
        "                \"collection_recovery_fee\", \"last_pymnt_amnt\", \"pub_rec_bankruptcies\"]\n",
        "\n",
        "updated_features = [feature for feature in updated_features if feature not in num_features]\n",
        "\n",
        "#print(len(updated_features))\n",
        "#print(loans.head())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgD2OhgGkoPI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# types of features need processing later\n",
        "categorical_features = [\"term\", \"grade\", \"sub_grade\", \"emp_length\", \"home_ownership\", \"verification_status\", \"purpose\", \"emp_title\", \"title\", \"addr_state\"] # remove \"emp_title\", \n",
        "num_features_in_str =[\"int_rate\", \"revol_util\"]\n",
        "text_features = [\"zip_code\", \"desc\"]\n",
        "date_features = [\"issue_d\", \"earliest_cr_line\", \"last_pymnt_d\", \"next_pymnt_d\", \"last_credit_pull_d\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKNOd-iArJGz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# remove the text_features = [\"zip_code\", \"desc\"] \n",
        "drop_features = [\"zip_code\", \"desc\"]\n",
        "loans.drop(drop_features, axis = 1, inplace = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1KzrT_0vGhQ",
        "colab_type": "text"
      },
      "source": [
        "The dataset is narrowed down into 39 columns. \n",
        "\n",
        "Till now, I haven't done any computation to the whole datase. I only did a rough general overview and cleaning to the whole dataset, and have some insights about how I will deal with the data later.  \n",
        "\n",
        "I will do the data cleaning, data engineering before splitting the training, validation, testing data to avoid data leakage. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e79YP3tXzyax",
        "colab_type": "text"
      },
      "source": [
        "## 3.3 Visualization of the features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_8I-5IXiJjD",
        "colab_type": "text"
      },
      "source": [
        "### 3.3.1 Visualization of target feature: loan_status"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBDnE-jqiOxq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loans.loan_status.value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ds0xISBtqI5o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loans.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBM21vtmWEhx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# only keep the rows loan_status = \"Fully Paid\" or \"Charged Off\"\n",
        "loans = loans.loc[loans[\"loan_status\"].isin([\"Fully Paid\", \"Charged Off\"])]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4T5KGepnuWHY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize = (6, 4))\n",
        "loans[\"loan_status\"].value_counts().plot(kind = \"bar\")\n",
        "plt.title(\"The distribution of target variable: loan_status\")\n",
        "plt.xticks(rotation = 0)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyVKx6wPIGJi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pvt = pd.pivot_table(loans[[\"loan_status\", \"term\"]], index = \"term\", columns = \"loan_status\", aggfunc = len)\n",
        "pvt.plot(kind = \"bar\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C56N5cp1i_Cd",
        "colab_type": "text"
      },
      "source": [
        "From the result above, the target variable **loan_status** is composed of two classes: Fully Paid (完全结清), & Charged Off (坏账注销). \n",
        "\n",
        "The dataset is imbalanced, and need further consideration later. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJ1sAnjhjEmg",
        "colab_type": "text"
      },
      "source": [
        "### 3.3.2 Visualization of the categorical features (分类变量的分布)\n",
        "\n",
        "探索典型分类变量依据好坏样本两类的分布并进行可视化展示："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sQrDfcsigos",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# list of the categorical/class features\n",
        "categorical_features = [\"term\", \"grade\", \"sub_grade\", \"emp_length\", \"home_ownership\", \"verification_status\", \"purpose\", \"emp_title\", \"title\", \"addr_state\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPYINiLAwwvB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for feature in categorical_features[:-3]:\n",
        "  pvt = pd.pivot_table(loans[[\"loan_status\", feature]], index = feature, columns = \"loan_status\", aggfunc = len)\n",
        "  ax = pvt.plot(kind = \"bar\", figsize = (8, 5))\n",
        "  plt.title(feature)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCvX1J39w8pj",
        "colab_type": "text"
      },
      "source": [
        "From the figure above, most people select the 36 months term. The percentage of 60 months loans are more likely to charged off.  \n",
        "借款人多选择３６期贷款，选择６０期贷款的违约率要高一些。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwLd5_DWrzpB",
        "colab_type": "text"
      },
      "source": [
        "From the grade, and subgrade, the shape of the two features are very similar, but the sub_grade are with more classes. **Consider drop the sub_grade feature**. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j90r8TH7kKUB",
        "colab_type": "text"
      },
      "source": [
        "由上图：\n",
        "\n",
        "我们需要首先理解一下grade这个特征。grade是ＬＣ自评等级，不同的网贷平台投资人有不一样的风险收益偏好，ＬＣ平台依据这种多样化的需求，将借贷人分成Ａ－Ｇ七个等级。\n",
        "\n",
        "ＬＣ使用复杂的算法对每笔贷款予以评级，这个评级和借款人的利率息息相关（这也说明，grade与某些特征是存在关系的）。比如说，那些信用历史好，还款能力好的借款人利率偏低，约７％，其贷款等级通常为Ａ级。从Ａ到Ｇ，贷款的风险越来越高，利率也越来越高。\n",
        "\n",
        "从图中，我们也可以看出这个趋势。\n",
        "\n",
        "另外，无论是投资人还是借贷者，大多数都是选择较低风险较低收益的类型。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQOVznL6wuKW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGOPCeJtkvr_",
        "colab_type": "text"
      },
      "source": [
        "由上图：\n",
        "\n",
        "工作时长低于１年（包括１年）与１０年以上的借款人最多，两者的违约比例相差不大，这张图打破了人们惯常以为的工作年限长就靠谱的想法。除此两类，在２－９年内，随着工作年限越长，贷款需求越少，可能是因为收入越来越稳定吧。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtygVseflqjT",
        "colab_type": "text"
      },
      "source": [
        "由上图：\n",
        "\n",
        "借款人住房按揭、租房最多，违约率不相上下；"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UduoQpCDtvrI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize = (16, 12))\n",
        "\n",
        "pvt = pd.pivot_table(loans[[\"loan_status\", \"addr_state\"]], index = \"addr_state\", columns = \"loan_status\", aggfunc = len)\n",
        "pvt.plot(kind = \"bar\", figsize=(12, 6))\n",
        "plt.xticks(rotation = 90, size = 10)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhqWJN9i6vGG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loans.columns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wz2BMeDzzDJI",
        "colab_type": "text"
      },
      "source": [
        "由上图：\n",
        "\n",
        "表征收入或收入来源是否经过核实。大部分借款是经过核实的，经过简单计算可知，核实的借款违约率约为0.15，未经核实的违约率约为0.14.可以说明网上提交的申请数据还是比较诚实的。另外，也可以初步推断，这个变量的预测能力应该不是特别强。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfH9PnXDzfil",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# update the features in categorical_features, which need encoding later, \n",
        "\n",
        "categorical_features = [\"term\", \"grade\", \"emp_length\", \"home_ownership\", \"verification_status\", \"emp_title\", \"purpose\", \"addr_state\"] # remove \"sub_grade\"\n",
        "\n",
        "loans.drop(\"sub_grade\", axis = 1, inplace = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mC-rMlU-2-Tl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# text features distribution\n",
        "# emp_title\n",
        "y_0 = loans[loans.loan_status == 'Fully Paid'].emp_title.value_counts()[:20].values\n",
        "y_1 = loans[loans.loan_status == 'Charged Off'].emp_title.value_counts()[:20].values\n",
        "ind = loans[loans.loan_status == 'Fully Paid'].emp_title.value_counts()[:20].index\n",
        "plt.figure(figsize = (8, 6))\n",
        "plt.bar(ind, y_0, color = 'g')\n",
        "plt.bar(ind, y_1, color = 'r')\n",
        "plt.title(\"The distribution of the loan_status vs emp_title\", size = 14, color = 'black')\n",
        "# set xticks vertical\n",
        "plt.xticks(ind, rotation = 'vertical') # \n",
        "plt.legend(['good', 'bad'])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-U0hlBu3OXE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# text features distribution\n",
        "# addr_state\n",
        "y_0 = loans[loans.loan_status == 'Fully Paid'].addr_state.value_counts()[:20].values\n",
        "y_1 = loans[loans.loan_status == 'Charged Off'].addr_state.value_counts()[:20].values\n",
        "ind = loans[loans.loan_status == 'Fully Paid'].addr_state.value_counts()[:20].index\n",
        "plt.figure(figsize = (10, 6))\n",
        "plt.bar(ind, y_0, color = 'g')\n",
        "plt.bar(ind, y_1, color = 'r')\n",
        "plt.title(\"The distribution of the loan_status vs addr_state\", size = 14, color = 'black')\n",
        "# set xticks vertical\n",
        "plt.xticks(ind, rotation = 'vertical') # \n",
        "plt.legend(['good', 'bad'])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxEaIiVx3U4d",
        "colab_type": "text"
      },
      "source": [
        "From the results above：\n",
        "\n",
        "The majority of the borrowers are from CA, NY, and TX, while most of the borrowers are from US Army, Bank of America, IMB, USAF, etc.\n",
        "\n",
        "借款人的区域来源，主要集中在加州、德州、纽约等等这些大州，后续数据处理可以考虑将这些主要区域提取出来。\n",
        "\n",
        "职务头衔：借款人的职务背景按照数量分布依次是公司、军队、医院。。。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGviWop2l2GR",
        "colab_type": "text"
      },
      "source": [
        "### 3.3.3 Visualization of the numerical features (连续值特征分布)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "styW5IIo0V3D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_features = [\"loan_amnt\", \"funded_amnt\", \"funded_amnt_inv\", \"installment\", \"annual_inc\", \"dti\", \"delinq_2yrs\", \"inq_last_6mths\", \"mths_since_last_delinq\", \"mths_since_last_record\",\n",
        "               \"open_acc\", \"pub_rec\", \"revol_bal\", \"total_acc\", \"total_pymnt\", \"total_pymnt_inv\", \"total_rec_prncp\", \"total_rec_int\", \"total_rec_late_fee\", \"recoveries\",\n",
        "                \"collection_recovery_fee\", \"last_pymnt_amnt\", \"pub_rec_bankruptcies\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJ9S81kSit2w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i, fea in enumerate(num_features[:5]):\n",
        "  plt.figure(figsize = (14, 16 * len(num_features[:5])))\n",
        "  plt.subplot(len(num_features), 1, i + 1)\n",
        "  df_temp_0 = loans[fea][loans.loan_status == 'Fully Paid']\n",
        "  df_temp_1 = loans[fea][loans.loan_status == 'Charged Off']\n",
        "  sns.distplot(df_temp_0.dropna(), color = 'g')\n",
        "  sns.distplot(df_temp_1.dropna(), color = 'r')\n",
        "  plt.title(fea, size = 12)\n",
        "  plt.legend(['Fully Paid', 'Charged Off'])\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbYAiKc-nhDh",
        "colab_type": "text"
      },
      "source": [
        "从这些图中可以初步得出如下信息：\n",
        "\n",
        "贷款额度、分期付款金额成有些长尾的正态分布，说明贷款额度集中在中小额度，但是也有分散的大额度\n",
        "\n",
        "年收入集中在０－１０万刀以内，但是也有极高收入（最高达到６００万）的借款人\n",
        "\n",
        "负债率较符合正态分布，但是高负债率相较低负债率违约风险更大\n",
        "\n",
        "分析过去２年内的违约次数分布，即便１次违约记录都没有，这次也可能会出现违约，\n",
        "\n",
        "过去6个月内查询次数越多，违约的概率越大\n",
        "\n",
        "迄今为止收到的付款总额或本金越少，违约率越高，这也显示了贷中监控的重要性，有问题及时预警。\n",
        "\n",
        "。。。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6Mnbztynzfx",
        "colab_type": "text"
      },
      "source": [
        "### 3.3.4 Time Series Distribuiotn (时序特征分布)\n",
        "全部时序特征：issue_d:（款发放月份），earliest_cr_line（首开信用卡时间），last_pymnt_d（最近一次收到还款的时间), last_credit_pull_d（ＬＣ撤回信贷最近的月份），其中第１，３，４ 项都是贷后数据。\n",
        "\n",
        "consider compute the time different between **issue_d** and **earliest_cr_line** to create new feature for time duration\n",
        "\n",
        "**consider drop** the 2 features to avoid data leakage: \n",
        "\n",
        "- last_pymnt_d\n",
        "- last_credit_pull_d"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "in0Mo2dH1Uzy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "date_features = [\"issue_d\", \"earliest_cr_line\", \"last_pymnt_d\", \"last_credit_pull_d\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l16g5oHvmysu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# timestamp features distribution\n",
        "df_time = loans[['issue_d', 'loan_status']].dropna()\n",
        "df_time.issue_d = [pd.datetime.strptime(str(j), '%b-%Y')\n",
        "                           for j in df_time.issue_d.values]\n",
        "\n",
        "pvt = pd.pivot_table(df_time, index = 'issue_d', columns = 'loan_status', aggfunc = len)\n",
        "pvt.plot(kind = 'bar')\n",
        "plt.xticks([])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qm2iYXW12Cis",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# timestamp features distribution\n",
        "df_time = loans[['earliest_cr_line', 'loan_status']].dropna()\n",
        "df_time.earliest_cr_line = [pd.datetime.strptime(str(j), '%b-%Y')\n",
        "                           for j in df_time.earliest_cr_line.values]\n",
        "\n",
        "pvt = pd.pivot_table(df_time, index = 'earliest_cr_line', columns = 'loan_status', aggfunc = len)\n",
        "pvt.plot(kind = 'bar')\n",
        "plt.xticks([])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7tiZdzUvLx4",
        "colab_type": "text"
      },
      "source": [
        "### 3.2.6 The correlation of each two numerical features (两两特征的协方差)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYJjUX9quBbc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# feature's correlation\n",
        "plt.figure(figsize = (14, 12))\n",
        "sns.heatmap(loans.iloc[:, :50].corr(), linewidths=.5 ) # annot = True\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlcJxqfgwMl4",
        "colab_type": "text"
      },
      "source": [
        "上图中浅颜色的部分是表示特征相关度高，除了对角线的区域，其它区域也分布着高相关性特征对，这说明样本集中某些特征之间存在强线性相关性，这个问题在选用某些机器学习模型（比如基于线性回归的模型族）时会显著影响模型性能，需要引起注意。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WeeLeT6f4tVy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loans.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVoNs5h9wOFI",
        "colab_type": "text"
      },
      "source": [
        "# 4. Data preprocesing (数据预处理)\n",
        "\n",
        "Based on the overview, after the first round data cleaning:\n",
        "number of features =  40, sample size = 39786\n",
        "** The features need processing:**\n",
        "\n",
        "- categorical_feaures: need encoding\n",
        " - term\"\n",
        " - \"grade\"\n",
        " - \"sub_grade\"\n",
        " - \"emp_length\"\n",
        " - \"home_ownership\"\n",
        " - \"verification_status\" \n",
        " - \"purpose\"\n",
        " - \"emp_title\"\n",
        " - \"title\"\n",
        " - \"addr_state\n",
        "\n",
        "- there is a high percentage of missing values: need replace with other values, or drop\n",
        "- some numerical features are represented with special signs, e.g., %\n",
        "  - \"int_rate\"\n",
        "  - \"revol_util\"\n",
        "- some of the features are correlated, need pca, or remove one or more\n",
        "\n",
        "- etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZgLEFTC8cmK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# types of features need processing later\n",
        "\n",
        "categorical_features = [\"term\", \"grade\", \"sub_grade\", \"emp_length\", \"home_ownership\", \"verification_status\", \"purpose\", \"emp_title\", \"title\", \"addr_state\"] # remove \"emp_title\", \n",
        "num_features_in_str =[\"int_rate\", \"revol_util\"]\n",
        "text_features = [\"zip_code\", \"desc\"]\n",
        "date_features = [\"issue_d\", \"earliest_cr_line\", \"last_pymnt_d\", \"next_pymnt_d\", \"last_credit_pull_d\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOGB4pC6wU_t",
        "colab_type": "text"
      },
      "source": [
        "## 4.1. training, validation, testing data seperation (数据集划分)\n",
        "To avoid data leakage, it's better to split the datasets before data preprocessing. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2Ck2CZLvqmJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# split the data\n",
        "x_col = loans.columns.tolist()\n",
        "x_col.remove('loan_status') # remove target variable from x\n",
        "X = loans[x_col]\n",
        "y = loans['loan_status']\n",
        "\n",
        "# 64% training data, 16% validation data, 20% testing data\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size = 0.2, random_state = 1)\n",
        "\n",
        "df_train = pd.concat([x_train, y_train], axis = 1)\n",
        "df_val = pd.concat([x_val, y_val], axis = 1)\n",
        "df_test = pd.concat([x_test, y_test], axis = 1)\n",
        "\n",
        "df_list = [df_train, df_val, df_test]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7mRE_U_8raY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"The shape of training dataset: {}\".format(df_train.shape))\n",
        "print(\"The shape of validation dataset: {}\".format(df_val.shape))\n",
        "print(\"The shape of testing dataset: {}\".format(df_test.shape))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkVF6Ln3xWlU",
        "colab_type": "text"
      },
      "source": [
        "## 4.2 Missing values replacement, process (特征缺失值识别与处理)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "It2LYIILxauj",
        "colab_type": "text"
      },
      "source": [
        "### 4.2.1 Features with high percentage of missing values (严重缺失值的处理)\n",
        "\n",
        "Identify the features with 60% more missing values and process \n",
        "找到缺失超过６０％的特征，并进一步了解缺失情况"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zju7LnAaxT5Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# default values\n",
        "# select features which 60% of the data is missing\n",
        "mis_feasures = [i for i in loans.columns if ((loans[i].isnull().sum())*1.0/loans.shape[0]) > 0.6]\n",
        "# output missing rate to get a further understanding of missing info\n",
        "for i in mis_feasures:\n",
        "  mis_rate = (loans[i].isnull().sum()) * 1.0 / loans.shape[0]\n",
        "  print(i, \"\\t\", mis_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "339Z6aphyJxw",
        "colab_type": "text"
      },
      "source": [
        "From the result above, after the first round data cleaning, the above two featurs have more than 60% percentages of missing values. \n",
        "\n",
        "From the experience, the two features **mths_since_last_delinq（距离上次违约的月份数）、mths_since_last_record（距离上一次公共黑记录月份数）** should be helpful to the loan model evaluation. Thus, even though these two features have high percentages of missing values, I will keep these two features, and replace the missing values approapately. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9K0rkHvUzhcc",
        "colab_type": "text"
      },
      "source": [
        "### 4.2.2 Missing values replacement (缺失值填充)\n",
        "\n",
        "了解现有缺失值的情况，根据缺失比例，缺失值的具体信息含义，缺失特征性质，来决定处理策略："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhiPhFpWy57F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define missing value features to be filled\n",
        "mis_feaures_to_fill = [i for i in loans.columns if loans[i].isnull().sum() != 0]\n",
        "for i in mis_feaures_to_fill:\n",
        "  mis_rate = (loans[i].isnull().sum() * 1.0/loans.shape[0])\n",
        "  print(i, '\\t', mis_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyfLggWj0fCv",
        "colab_type": "text"
      },
      "source": [
        ">Features | Type | Missing pct | Fill strategy\n",
        ">--- | --- | --- | ---\n",
        ">emp_title | categorical | 6.2% | mode\n",
        ">emp_length | categorical | 2.7% | mode\n",
        ">title | categorical | 0.027% | mode\n",
        ">mths_since_last_deling | numerical | 64.66% | missing value as one class\n",
        ">mths_since_last_record | numerical | 92/98% | missing value as one class\n",
        ">revol_util | categorical | 0.126% | mode\n",
        ">last_pymnt_d | categorical | 0.178% | after approval of the loan, **drop**\n",
        ">last_credit_pull_d | categorical | 0.005% | after approval of the loan, **drop**\n",
        ">pub_rec_bankrupticies | categorical | 0.175% | mode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FuRf_Oge5a3a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# drop the last_pymnt_d & last_credit_pull_d\n",
        "for data in df_list:\n",
        "  data.drop([\"last_pymnt_d\", \"last_credit_pull_d\"], axis = 1, inplace = True)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_neigGX0Fafq",
        "colab_type": "text"
      },
      "source": [
        "#### 4.2.2.1 Fill with mode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUFK4XOH0kTQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy.stats import mode\n",
        "\n",
        "# fill with mode number\n",
        "features_fill_with_mode = [\"emp_title\", \"emp_length\", \"title\", \"revol_util\", \"pub_rec_bankruptcies\"]\n",
        "\n",
        "for data in df_list:\n",
        "  for i in features_fill_with_mode:\n",
        "    data[i][data[i].isnull()] = mode(data[i][data[i].notnull()])[0][0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sTbhdG_1lBH",
        "colab_type": "text"
      },
      "source": [
        "注：\n",
        "\n",
        "由于接下来将用到逻辑回归模型，它对特征线性相关性较敏感，所以尽管用其它变量拟合缺失值对缺失值的填充会更符合实际情况，也没有采用这种方法。\n",
        "\n",
        "为什么使用众数填充而不是用中位数或是均值填充？因为众数对于数值型变量和字符型变量都适用，而且也有统计意义。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q50_-eEF4ZUF",
        "colab_type": "text"
      },
      "source": [
        "### 4.3 Features with the same values\n",
        "At the very beginning, the features with 100% missing values or 100% same values are removed. \n",
        "\n",
        "Next, I will focus on the other imbalanced features. \n",
        "In general, if the percentage of the featuers with 90% more same values will be problematic. \n",
        "\n",
        "如果一个变量大部分的观测都是相同的特征，那么这个特征或者输入变量就是无法用来区分目标时间，一般来说，临界点在90%。但是最终的结果还是应该基于业务来判断。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ooIVkXOm1lcv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 同值性特征识别处理，将阀值设定为90%\n",
        "same_value_features = []\n",
        "for i in loans.columns:\n",
        "  try:\n",
        "    mode_value = mode(loans[i])[0][0]\n",
        "    mode_rate = mode(loans[i])[1][0]*1.0 / loans.shape[0]\n",
        "    if mode_rate > 0.9:\n",
        "      same_value_features.append([i, mode_value, mode_rate])\n",
        "  except:\n",
        "    pass\n",
        "  \n",
        "temp = pd.DataFrame(same_value_features, columns = ['col_name', 'mode_value', 'mode_rate'])\n",
        "temp.sort_values(by = 'mode_rate')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVXfMPdRJmLG",
        "colab_type": "text"
      },
      "source": [
        ">Features | Mode_value | Mode_rate | Strategy\n",
        ">--- | --- | --- | ---\n",
        ">collection_recovery_fee | 0 | 90% | after charged off, **drop**\n",
        ">pub_rec | 0 | 94.7% | most borrowers don't have public recovery record, seems reasonable, **drop**\n",
        ">total_rec_late_fee | 0 | 94.77% | after charged off, **drop**\n",
        ">pub_rec_bankruptcies | 0 | 95.57% | most borrowers don't have public bankrupticy record, seems reasonable, **drop**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyDCn8D3LVwo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# remove the features from the training, validation, testing datasets\n",
        "for data in df_list:\n",
        "  data.drop([\"collection_recovery_fee\", \"pub_rec\", \"total_rec_late_fee\", \"pub_rec_bankruptcies\"], axis = 1, inplace = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRKD-1U0_Idv",
        "colab_type": "text"
      },
      "source": [
        "### 4.4 Features format preprocessing\n",
        "\n",
        "- #### 4.4.1. format numerical features, remove % sign & transform string to num\n",
        "  - \"int_rate\" : \n",
        "  - \"revol_util\" : \n",
        "  - \"term\" : e.g. \" 36 months\" --> 36\n",
        "\n",
        " \n",
        "\n",
        "- #### 4.4.2. format time features\n",
        "  - \"issue_d\"\n",
        "  - \"earliest_cr_line\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0rDEXdsA3yq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# update the features which need further processing later\n",
        "\n",
        "#string, text features, need recoding\n",
        "categorical_features = [\"term\", \"grade\", \"emp_length\", \"home_ownership\", \"verification_status\", \"purpose\", \"emp_title\", \"title\", \"addr_state\"]  \n",
        "# numerical values with % sign, need recoding\n",
        "\n",
        "num_features_in_str =[\"int_rate\", \"revol_util\"]\n",
        "\n",
        "# date features\n",
        "date_features = [\"issue_d\", \"earliest_cr_line\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYheo5gz7Lhh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#df_train.describe(include = \"all\").T\n",
        "df_train.head(3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qPVFBBqRwAd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for data in df_list:\n",
        "  data['term'] = data['term'].replace(\" 36 months\", 36).replace(\" 60 months\", 60).astype('float')\n",
        "  \n",
        "  # remove %\n",
        "  data['int_rate'] = data['int_rate'].str.replace('%', '').astype('float')\n",
        "  data['revol_util'] = data['revol_util'].str.replace('%', '').astype('float')\n",
        "  \n",
        "  # format time features\n",
        "  data['earliest_cr_line'] = [pd.datetime.strptime(str(j), '%b-%Y') for j in data['earliest_cr_line']]\n",
        "  data['issue_d'] = [pd.datetime.strptime(str(j), '%b-%Y') for j in data['issue_d']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUQUzj1He0rd",
        "colab_type": "text"
      },
      "source": [
        "### 4.5 Text features processing (文本特征处理)\n",
        "- emp_title\n",
        "- title\n",
        "- addr_state\n",
        "\n",
        "经过上述步骤处理后，现有的文本类数据包括：emp_title(职务信息），title(标题）,addr_state(借款人地址），其中title与purpose相关性较强，都是表明借款用途的信息。** consider grop title**\n",
        "\n",
        "emp_title字面上看是职务头衔，但是实际内容是借款人所在机构，它类型多，且是文本型特征，但是根据数据探索阶段得到的结论，这个变量含有预测性的信息，用模型分箱相当耗费性能，所以根据经验考虑尝试机构类别将它分类。\n",
        "\n",
        "addr_state也包含预测性信息，可以尝试用卡方分箱或依据经验进行进行分类"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcFcdvrzfs1T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for data in df_list:\n",
        "  data.drop(\"title\", axis = 1, inplace = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BBrKqpTfaW-",
        "colab_type": "text"
      },
      "source": [
        "### 4.5.1 emp_title 工作机构分类 (暂时去掉)\n",
        "\n",
        "Since there are 28862 distinct titles, **consider drop** it temperally.\n",
        "\n",
        "工作机构分类，依据A政府机构类，B银行类，F医院类，E学校类，C自职业类，D公司和其它类，G退休类分类． 如果数据中的emp_title与某个上述A-G有交集，则将它划为该类，用字母字符表示； 缺省值为’H'。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXmrkN9zV-10",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for data in df_list:\n",
        "  data.drop(\"emp_title\", axis = 1, inplace = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfBLyMJIuHsr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\"\"\"\n",
        "# 工作机构分类，依据A政府机构，B银行， F医院类， E学校类， C自由职业类，D公司和其它类， G退休类\n",
        "# 如果loans中emp_title与某个上述工作有交集， 则将其化为该类， 用字母表示， \n",
        "# 缺失值用”H\"代替\n",
        "\n",
        "A = ['board', 'general', 'american', 'U.S.', 'army', 'force', 'states', 'corp', 'navy', 'united', 'department']\n",
        "B = ['bank', 'morgan']\n",
        "C = ['self']\n",
        "D = 'OTHER'\n",
        "E = ['college', 'school', 'university']\n",
        "F = ['hospital', 'clinic', 'health', 'healthcare']\n",
        "G = ['retired']\n",
        "\n",
        "ls_letter = [0, 1, 2, 4, 5, 6]\n",
        "ls = [A, B, C, E, F , G]\n",
        "\n",
        "def emp_classify(df1):\n",
        "  for i in df1.emp_title.index:\n",
        "    emp_list = []\n",
        "    for j in range(len(ls)):\n",
        "      emp_list.append((set(str(df1.emp_title[i]).lower().split())&set(ls[j])))\n",
        "    if emp_list.count(set()) != 6:\n",
        "      sr_emp = pd.Series(emp_list)\n",
        "      idx = sr_emp[sr_emp != set()].index\n",
        "      df1.emp_title[i] = ls_letter[idx[0]]\n",
        "    else:\n",
        "      df1.emp_title[i] = 3\n",
        "  df1.emp_title[df1.emp_title.isnull()] = 7\n",
        "  \n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hKihJ_Ohwhm",
        "colab_type": "text"
      },
      "source": [
        "### 4.5.2 addr_state 借款人州地址分类 (暂时去掉)\n",
        "\n",
        "Since there are 50 distinct states, consider drop it temperally\n",
        "\n",
        "４．５．２．１　k-means聚类分析\n",
        "\n",
        "首先尝试使用k-means方式进行聚类，看看每一类中的州地址有没有什么共性："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvvZIiRy9PrW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for data in df_list:\n",
        "  data.drop(\"addr_state\", axis = 1, inplace = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6UMfna7hc_e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 尝试k-means 方法对所在州聚类\n",
        "# K-means 是基于距离值聚类，因此需要先归一化处理\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.cluster import KMeans\n",
        "mm = MinMaxScaler()\n",
        "\n",
        "\"\"\"\n",
        "df_addr_temp.iloc[:, :] = mm.fit_transform(df_addr_temp)\n",
        "\n",
        "km = KMeans(n_clusters = 8, random_state = 3)\n",
        "km.fit(df_addr_temp.values)\n",
        "df_addr_temp = pd.concat([df_addr_temp, df_train.addr_state, pd.Series(km.labels_)], axis = 1)\n",
        "\n",
        "df_addr_temp.groupby([0, 'addr_state'])['addr_state'].count()\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4RHEaD0jB9P",
        "colab_type": "text"
      },
      "source": [
        "从输出结果中并没有看出不同群体有什么共性。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0L87cIvcIld",
        "colab_type": "text"
      },
      "source": [
        "### 4.6 时序特征处理\n",
        "将earliest_cr_line, issue_d 的字符串格式数据，转换为标准datetime格式"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYCgfU8dk6iQ",
        "colab_type": "text"
      },
      "source": [
        "### 4.6 Categorical features recoding (特征编码)\n",
        "\n",
        "逻辑回归和随机森林模型不接受字符型变量，因此需要将对此类变量进行编码。\n",
        "\n",
        "常用的编码方式有类别标签法（不同的类别映射到不同的数值），哑变量编码法（对类别变量取哑变量）等等。考虑到评分卡模型的简洁性，在此选用类别标签法。。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgpeGmg1c6P6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# encoding features: grade, emp_length, home_ownership, verification_status, purpose\n",
        "import numpy as np\n",
        "\n",
        "for data in df_list:\n",
        "  \n",
        "  # feature grade\n",
        "  data.grade.replace({\"A\":0, \"B\":1, \"C\": 2, \"D\": 3, \"E\": 4, \"F\": 5, \"G\": 6}, inplace = True)\n",
        "  \n",
        "  # feature emp_length\n",
        "  data.emp_length.replace({\"< 1 year\": 1, \"1 year\": 2, \"2 years\": 3, \"3 years\": 4, \"4 years\": 5, \"5 years\": 6, \"6 years\": 7, \"7 years\": 8, \"8 years\": 9,\n",
        "                           \"9 years\": 10, \"10+ years\": 11}, inplace = True)\n",
        "  \n",
        "  # feature home_ownership\n",
        "  data.home_ownership.replace({\"MORTGAGE\": 0, \"OTHER\": 1, \"NONE\": 2, \"OWN\": 3, \"RENT\": 4}, inplace = True)\n",
        "  \n",
        "  # feature verification_status\n",
        "  data.verification_status.replace({'Not Verified': 0, \"Source Verified\": 1, \"Verified\": 2}, inplace = True)\n",
        "  \n",
        "  # feature purpose\n",
        "  data.purpose.replace({\"credit_card\": 0, \"home_improvement\": 1, \"debt_consolidation\": 2, \"other\": 3, \n",
        "                    \"major_purchase\": 4, \"medical\": 5, \"small_business\": 6, \n",
        "                    \"car\": 7, \"vacation\": 8, \"moving\": 9, \"house\": 10,\n",
        "                    \"renewable_energy\": 11, \"wedding\": 12, \"education\": 13, \"educational\": 13}, inplace = True)\n",
        "  \n",
        "  # feature loan_status\n",
        "  data.loan_status.replace({\"Fully Paid\": 0, \"Charged Off\": 1}, inplace = True)\n",
        "  \n",
        "  # feature term\n",
        "  data.term.replace({36.0: 0, 60.0: 1}, inplace = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JN62rRBSdMne",
        "colab_type": "text"
      },
      "source": [
        "### 4.7 Data normalization 归一化处理\n",
        "\n",
        "逻辑回归模型基于线性回归，求参需要用到梯度下降法，为了加快迭代速度，不同特征的变化范围规模相差不宜过大，如果用数值直接带入逻辑回归模型，必须进行变量缩放。但是本文是用逻辑回归建立评分卡，会将数值变量进行分箱，所以这一步可以省略。\n",
        "\n",
        "### 4.8 Be careful about data leakage (警惕数据泄露)\n",
        "\n",
        "数据泄露分为２种：\n",
        "1. 不恰当特征导致的泄露, 例如用已经同意贷款一周的特征，因此， 只有同意贷款了的人有这些特征， 没有同意贷款的人没有这一特征，就会导致信息泄露、\n",
        "2. 不恰当的交叉验证策略导致泄露， 例如， 在training, validatiaon, testing data 确定分离之前进行归一化或标准化就相当于已知validation, 和testing 的数据"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-lCW_ZudX8w",
        "colab_type": "text"
      },
      "source": [
        "### 4.8.1 警惕不恰当特征\n",
        "\n",
        "所有特征中，一旦在目标属性出现后，会随之更新或出现的属性，属于会泄露信息的属性，在这个数据集中，包括贷中、贷后特征。\n",
        "\n",
        "删除此类特征（之前预处理步骤中已经删除了一些）："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkthSlI-hAYr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train.columns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pkLBfODdb5d",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EP7IZgemdcN8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# delete leaking features & not meaningful features \n",
        "leak_feas = ['recoveries', 'last_pymnt_amnt', 'funded_amnt', 'funded_amnt_inv', 'total_pymnt',\n",
        "            'total_pymnt_inv', 'total_rec_prncp', 'total_rec_int']\n",
        "\n",
        "for data in df_list:\n",
        "  data.drop(leak_feas, axis = 1, inplace = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUk5v4IJdf9T",
        "colab_type": "text"
      },
      "source": [
        "### 4.8.2 错误的交叉验证策略：\n",
        "尽量保证验证集数据的纯粹，不要让它参与到训练集的处理和模型构建当中，这意味着，要在预处理之前分割训练集和测试集。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHCT7P_4bHDf",
        "colab_type": "text"
      },
      "source": [
        "## 5. 特征工程\n",
        "### 5.1 特征衍生\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21YvyDGojDdP",
        "colab_type": "text"
      },
      "source": [
        "### 4.5.3 等频分箱 (quantile)\n",
        "常用的分箱法包括卡方分箱，等频或等距，聚类，依据经验分箱等。\n",
        "在此采用有监督的最优分箱法-卡方分箱。\n",
        "\n",
        "\n",
        "用频数进行分类，使每一个分组内的样本数尽量相近\n",
        "\n",
        "等宽分箱（uniform）\n",
        "\n",
        "聚类分箱（Kmeans）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XneHvAiLWIQD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#卡方分箱\n",
        "def Chi2(df, total_col, bad_col, overallRate):\n",
        "  df2 = df.copy()\n",
        "  df2[\"expected\"] = df[total_col].apply(lambda x: x * overallRate)\n",
        "  combined = zip(df2[\"expected\"], df2[bad_col])\n",
        "  chi = [(i[0] - i[1]) ** 2 / i[0] for i in combined]\n",
        "  chi2 = sum(chi)\n",
        "  return chi2\n",
        "\n",
        "def ChiMerge_MaxInterval_Original(df, col, target, max_interval = 5):\n",
        "  colLevels = set(df[col])\n",
        "  colLevels = sorted(list(colLevels))\n",
        "  N_distinct = len(colLevels)\n",
        "  if N_distinct <= max_interval:\n",
        "    print(\"The number of rows cannot be less than interval numbers\")\n",
        "    return colLevels[:-1]\n",
        "  else:\n",
        "    total = df.groupby([col])[target].count()\n",
        "    total = pd.DateFrame({\"total\": total})\n",
        "    bad = df.groupby([col])[target].sum()\n",
        "    bad = pd.DataFrame({\"bad\": bad})\n",
        "    regroup = total.merge(bad, left_index = True, right_index = True, how = \"left\")\n",
        "    regroup.reset_index(level = 0, inplace = True)\n",
        "    N = sum(regroup[\"total\"])\n",
        "    B = sum(regroup[\"bad\"])\n",
        "    overallRate = B*1.0/N\n",
        "    groupIntervals = [[i] for i in colLevels]\n",
        "    groupNum = len(groupIntervals)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6z4473GaiOL7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 尝试用频数经行分类， 是每一个分组内的样本数尽量相近， 假设8个样本， 每个分组大约3300个样本\n",
        "\n",
        "from sklearn.preprocessing import KBinsDiscretizer\n",
        "#x = loans.loc[:, \"addr_state\"].values.reshape(-1, 1)\n",
        "\n",
        "#est = KBinsDiscretizer(n_bins = 6, encode = \"onehot\", strategy = \"quantile\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QuD75_6tvnSq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "S= [['CA'], ['FL', 'TX'], ['NJ', 'PA', 'IL'], ['VA', 'GA', 'MA', 'OH'], ['MD', 'AZ', 'WA', 'CT', 'CO', 'NC'], \n",
        "   ['MI', 'MO', 'MN', 'NV', 'OR', 'WI', 'LA', 'SC', 'AL', 'OK']]\n",
        "\n",
        "addr_list = df_train.addr_state.value_counts().index.tolist()\n",
        "\n",
        "SS = []\n",
        "for i in S:\n",
        "  SS += i\n",
        "  \n",
        "S.insert(7, list(set(addr_list) - set(SS)))\n",
        "\n",
        "addr_set = []\n",
        "for i in S:\n",
        "  addr_set.append(set(i))\n",
        "  \n",
        "addr_dict = {}\n",
        "for i, j in zip(range(8), addr_set):\n",
        "  addr_dict[i] = j\n",
        "  \n",
        "# 将addr_state 进行分类转换\n",
        "\n",
        "def trans_add_func(df_to_trans):\n",
        "  for i in df_to_trans.addr_state[10:]:\n",
        "    for j in range(len(addr_dict)):\n",
        "      if i in addr_dict[j]:\n",
        "        df_to_trans.addr_state[df_to_trans.addr_state == i] = list(addr_dict.keys())[j]\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LIt8b9lo__B",
        "colab_type": "text"
      },
      "source": [
        "### 4.8.1 警惕不恰当特征\n",
        "\n",
        "所有特征中，一旦在目标属性出现后，会随之更新或出现的属性，属于会泄露信息的属性，在这个数据集中，包括贷中、贷后特征。\n",
        "\n",
        "删除此类特征（之前预处理步骤中已经删除了一些）：\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z72BLIGpprGA",
        "colab_type": "text"
      },
      "source": [
        "# 5. Feature Engineering\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3TozxArpvhQ",
        "colab_type": "text"
      },
      "source": [
        "### 5.1 Generated feature (特征衍生)\n",
        "**cre_hist = issue_d - earliest_cr_line**\n",
        "\n",
        "将时序变量衍生为月份值，将（贷款发放时间－首次使用信用卡时间）作为一个新的变量，表示信用历史(cre_hist)，单位是月份。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZlLc4iipF49",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# deriving features, cre_hist = issue_d - earliest_cr_line\n",
        "\n",
        "for data in df_list:\n",
        "  data['cre_hist'] = [j.days for j in (data.issue_d - data.earliest_cr_line)/30]\n",
        "  data.drop(['issue_d', 'earliest_cr_line'], axis = 1, inplace = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVIQ49sYqHAz",
        "colab_type": "text"
      },
      "source": [
        "### 5.2 筛选变量"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9CpQKUgqLDq",
        "colab_type": "text"
      },
      "source": [
        "### 5.2.1 依据共线性筛选变量\n",
        "\n",
        "逻辑回归是基于线性回归模型，其前提假设是用于建模的特征之间不存在线性相关性，因此，它对共线性问题比较敏感。共线性的存在对模型稳定性有很大影响，并且也无法区分每个特征对目标变量的解释性。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GU5zhdZYqQFw",
        "colab_type": "text"
      },
      "source": [
        "依据VIF（方差膨胀系数）筛选变量\n",
        "\n",
        "每个特征的VIF计算是用其它特征对它进行回归拟合，如果这种拟合的解释性很强，说明它们之间存在多重共线性。\n",
        "\n",
        "用VIF计算连续型变量的共线性："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDZfRO2Wef0i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#df_train.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocHPpNqoCEi3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# calcuate vif\n",
        "\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor as vif\n",
        "\n",
        "# select number features \n",
        "vif_feas = [i for i in df_train.columns if i not in ['term', 'mths_since_last_delinq', 'mths_since_last_record', 'addr_state', 'emp_title', 'loan_status']]\n",
        "\n",
        "vif_ls = []\n",
        "\n",
        "for i in range(len(vif_feas)):\n",
        "  vif_ls.append([vif_feas[i], vif(df_train[vif_feas].values, i)])\n",
        "  \n",
        "vif_df = pd.DataFrame(vif_ls, columns = ['col_name', 'vif'])\n",
        "vif_df.sort_values(by = 'vif', ascending = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tn8zrL0grbad",
        "colab_type": "text"
      },
      "source": [
        "用协方差计算线性相关性："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5AEUfMSrfF5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# calculate correlation\n",
        "cor = df_train[vif_feas].corr()\n",
        "\n",
        "# get lower trianglar matrix of cor\n",
        "cor.iloc[:, :] = np.tril(cor.values, k = -1)\n",
        "\n",
        "# stack columns of cor\n",
        "cor = cor.stack()\n",
        "cor[np.abs(cor) > 0.7]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MwJwtbCrdp7",
        "colab_type": "text"
      },
      "source": [
        "根据经验，vif>10, cor>0.7,变量之间的有显著性的相关性。\n",
        "\n",
        "从上述关于协方差和VIF的分析结果中我们可以看到：installment & loan_amnt，int_rate & grade，total_acc&open_acc，pub_rec_bankruptcies & pub_rec之间存在线性关系，因为pub_rec_bankruptcies & pub_rec中大多数都是０，所以其有相关可以解释。剩下的几对，我们可以尝试先删除每对中的一个,删除installment & grade,再去检测相关系数。\n",
        "\n",
        "此外，有几个处于边界地带的特征，暂时留下，特征工程中删除特征时要谨慎，因为删除特征，意味着弃用一些信息。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgNWFZEkr7IW",
        "colab_type": "text"
      },
      "source": [
        "### Check the vif and correlation again after deleting the highly correlated features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMgmkp9qr7dz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in [\"installment\", \"grade\"]:\n",
        "  vif_feas.remove(i)\n",
        "  \n",
        "for data in df_list:\n",
        "  data.drop([\"installment\", \"grade\"], axis = 1, inplace = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMkDbibwrwCn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# calcuate vif\n",
        "\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor as vif\n",
        "\n",
        "# select number features \n",
        "vif_feas = [i for i in df_train.columns if i not in ['term', 'mths_since_last_delinq', 'mths_since_last_record', 'addr_state', 'emp_title', 'loan_status']]\n",
        "\n",
        "vif_ls = []\n",
        "\n",
        "for i in range(len(vif_feas)):\n",
        "  vif_ls.append([vif_feas[i], vif(df_train[vif_feas].values, i)])\n",
        "  \n",
        "vif_df = pd.DataFrame(vif_ls, columns = ['col_name', 'vif'])\n",
        "vif_df.sort_values(by = 'vif', ascending = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUxfu6qzr5HV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# calculate correlation\n",
        "cor = df_train[vif_feas].corr()\n",
        "\n",
        "# get lower trianglar matrix of cor\n",
        "cor.iloc[:, :] = np.tril(cor.values, k = -1)\n",
        "\n",
        "# stack columns of cor\n",
        "cor = cor.stack()\n",
        "cor[np.abs(cor) > 0.7]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAyD3xpPsKDM",
        "colab_type": "text"
      },
      "source": [
        "### 5.2.2 依据特征重要性筛选变量 (暂时跳过)\n",
        "\n",
        "筛选变量常用的方法有，基于正则化损失函数的线性模型，基于机器学习模型输出的特征重要性，基于IV值。\n",
        "\n",
        "在此，为了方便进行评分卡建模，采用IV值.\n",
        "\n",
        "定义woe和iv:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUnvTe5fswa9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# calcualte woe and IV \n",
        "\n",
        "def CalcWOE(df, col, target):\n",
        "  total = df.groupby([col])[target].count()\n",
        "  total = pd.DataFrame({\"total\": total})\n",
        "  bad = df.groupby([col])[target].sum()\n",
        "  bad = pd.DataFrame({\"bad\": bad})\n",
        "  regroup = total.merge(bad, left_index = True, right_index = True, how = \"left\")\n",
        "  regroup.reset_index(level = 0, inplace = True)\n",
        "  N = sum(regroup[\"total\"])\n",
        "  B = sum(regroup[\"bad\"])\n",
        "  regroup[\"good\"] = regroup[\"total\"] - regroup[\"bad\"]\n",
        "  G = N - B \n",
        "  regroup[\"bad_pcnt\"] = regroup[\"bad\"].map(lambda X: X * 1.0/B)\n",
        "  regroup[\"good_pcnt\"] = regroup[\"good\"].map(lambda X: X * 1.0/G)\n",
        "  \n",
        "  # WOE 在这里显示不同特征不同属性对好样本的预测能力， 这符合评分卡计分标准， 分数越高， 越可靠\n",
        "  regroup[\"WOE\"] = regroup.apply(lambda X: np.log(X.good_pcnt * 1.0 / X.bad_pcnt), axis = 1)\n",
        "  WOE_dict = regroup[[col, \"WOE\"]].set_index(col).to_dict()\n",
        "  IV = regroup.apply(lambda X: (X.good_pcnt - X.bad_pcnt) * np.log(X.good_pcnt * 1.0 / X.bad_pcnt), axis = 1)\n",
        "  IV_SUM = sum(IV)\n",
        "  return {\"WOE\": WOE_dict, \"IV_SUM\": IV_SUM, \"IV\": IV}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cos17UvQucBX",
        "colab_type": "text"
      },
      "source": [
        "计算每个变量的IV 值并按从大到小的顺序排序："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2YQtLNDuegw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# calcualte IV\n",
        "df_train_boxed = df_train.copy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cZYmG89ujRH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get IV\n",
        "iv_list = []\n",
        "for i in df_train_boxed.columns:\n",
        "  iv_dict = CalcWOE(df_train_boxed, i, \"loan_status\")\n",
        "  iv_list.append(iv_dict[\"IV_SUM\"])\n",
        "  \n",
        "# get IV form\n",
        "\n",
        "iv_df = pd.DataFrame({\"iv_name\": df_train_boxed.columns.values, \"iv\": iv_list})\n",
        "\n",
        "iv_df.sort_values('iv', ascending = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdLP5jZkvJNJ",
        "colab_type": "text"
      },
      "source": [
        "如果输出IV值是无穷，说明该特征中某些属性中缺失某类样本，这需要重新分箱，将这类样本添加到相邻类（对于连续型数值样本）或样本数量较少的那一类（对于分类样本）中去： deling_2yrs:把7.0, 8.0, 9.0, 11.0划归到7.0那一类,全算作6.0\n",
        "\n",
        "home_ownership:把2添加到1 这一类\n",
        "\n",
        "pub_rec：把3.0，4.0，添加到2.0这一类"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VglUiktDvNSx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#实施上述区间合并\n",
        "for df_i in df_list:\n",
        "  df_i.delinq_2yrs[df_i.delinq_2yrs.isin([7.0, 8.0, 9.0, 11.0])] = 6.0\n",
        "  df_i.home_ownership[df_i.home_ownership == 2] = 1\n",
        "  #df_i.pub_rec[df_i.pub_rec.isin([3.0, 4.0])] = 2.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gdTppfE4NE4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train.home_ownership.value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upHN4Rx3vpZv",
        "colab_type": "text"
      },
      "source": [
        "保留IV大于0.15的特征："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPen_6Nivruv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 保留IV值> 0.015的变量\n",
        "valid_feas = iv_df[iv_df.iv > 0.015].iv_name.tolist()\n",
        "valid_feas.remove('loan_status')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ma29ia7Ivr6T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "valid_feas"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4VUCK27v-hW",
        "colab_type": "text"
      },
      "source": [
        "### 5.3 features bin (特征分箱)\n",
        "特征分箱就是把连续特征转化为离散特征，或者减小离散特征的离散性。\n",
        "\n",
        "特征分箱有如下好处：\n",
        "\n",
        "特征分箱后，特征被简化，也简化了模型，比如在逻辑回归评分卡模型中，评分卡被简化，基于决策树的模型中，决策树枝杈减少，降低了过拟合的风险，有效增加了模型的稳定性。\n",
        "\n",
        "特征分箱可以将缺失值划为一类，比如此样本中无法被编码的公共记录缺失类。\n",
        "\n",
        "特征离散化后对异常数据也有更强的容错性。比如假设年龄数据中出现１０００岁，模型可以自动将其划分为>80岁一类，否则它会对对异常值敏感的模型如（逻辑回归）造成很大影响。\n",
        "\n",
        "特征离散化之后，方便进一步进行非线性的特征衍生。\n",
        "\n",
        "### 4.5.3 等频分箱 (quantile)\n",
        "常用的分箱法包括卡方分箱，等频或等距，聚类，依据经验分箱等。\n",
        "在此采用有监督的最优分箱法-卡方分箱。\n",
        "\n",
        "\n",
        "用频数进行分类，使每一个分组内的样本数尽量相近\n",
        "\n",
        "等宽分箱（uniform）\n",
        "\n",
        "聚类分箱（Kmeans）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUpi646-wIpJ",
        "colab_type": "text"
      },
      "source": [
        "分箱的方法：\n",
        "\n",
        "常用的分箱方法包括卡方分箱、等频或等距分箱、聚类、依据经验分箱等。\n",
        "\n",
        "对连续型数值变量，在此采用有监督的最优分箱法——卡方分箱。\n",
        "\n",
        "定义卡方分箱函数："
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkzP3eefyaoG",
        "colab_type": "text"
      },
      "source": [
        "### 1. 卡方分箱"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTVw5gGovr0x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 卡方分箱\n",
        "def Chi2(df, total_col, bad_col, overallRate):\n",
        "  # df : dataframe\n",
        "  # total_col: 每个值的总数量\n",
        "  # bad_col: 每个值的坏数据量 \n",
        "  # overallRate: 坏数据的占比\n",
        "  df2 = df.copy()\n",
        "  df2[\"expected\"] = df[total_col].apply(lambda X: X * overallRate)\n",
        "  combined = zip(df2[\"expected\"], df2[bad_col])\n",
        "  chi = [(i[0] - i[1]) ** 2/i[0] for i in combined]\n",
        "  chi2 = sum(chi)\n",
        "  return chi2\n",
        "  \n",
        "def ChiMerge_MaxInterval_original(df, col, target, max_interval = 5):\n",
        "\n",
        "  '''\n",
        "  : df dataframe\n",
        "  : col 要被分项的特征\n",
        "  ： target 目标值 0,1 值\n",
        "  : max_interval 最大箱数\n",
        "  '''\n",
        "    \n",
        "  colLevels = set(df[col])\n",
        "  colLevels = sorted(list(colLevels))\n",
        "  N_distinct = len(colLevels)\n",
        "  if N_distinct <= max_interval:\n",
        "    print(\"The row number cann't be less than interval numbers\")\n",
        "    return colLevels[:-1]\n",
        "  \n",
        "  else:\n",
        "    total = df.groupby([col])[target].count()\n",
        "    total = pd.DataFrame({\"total\": total})\n",
        "    bad = df.groupby([col])[target].sum()\n",
        "    bad = pd.DataFrame({'bad':bad})\n",
        "    regroup = total.merge(bad, left_index = True, right_index = True, how = \"left\")\n",
        "    regroup.reset_index(level = 0, inplace = True)\n",
        "    N = sum(regroup[\"total\"])\n",
        "    B = sum(regroup[\"bad\"])\n",
        "    overallRate = B * 1.0 / N \n",
        "    groupIntervals = [[i] for i in colLevels]\n",
        "    groupNum = len(groupIntervals) \n",
        "    \n",
        "    while(len(groupIntervals)>max_interval):\n",
        "      chisqList=[]\n",
        "      for interval in groupIntervals:\n",
        "          df2=regroup.loc[regroup[col].isin(interval)]\n",
        "          chisq=Chi2(df2,'total','bad',overallRate)\n",
        "          chisqList.append(chisq)\n",
        "      min_position=chisqList.index(min(chisqList))\n",
        "      if min_position==0:\n",
        "          combinedPosition=1\n",
        "      elif min_position==groupNum-1:\n",
        "          combinedPosition=min_position-1\n",
        "      else:\n",
        "          if chisqList[min_position-1]<=chisqList[min_position + 1]:\n",
        "              combinedPosition=min_position-1\n",
        "          else:\n",
        "              combinedPosition=min_position+1\n",
        "      #合并箱体\n",
        "      groupIntervals[min_position]=groupIntervals[min_position] + groupIntervals[combinedPosition]\n",
        "      groupIntervals.remove(groupIntervals[combinedPosition])\n",
        "      groupNum=len(groupIntervals)\n",
        "\n",
        "  groupIntervals=[sorted(i) for i in groupIntervals]\n",
        "  print (groupIntervals)\n",
        "  cutOffPoints=[i[-1] for i in groupIntervals[:-1]]\n",
        "  return cutOffPoints"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ha4cqZMl5aF6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ChiMerge_MaxInterval_original(df_train, \"loan_amnt\", \"loan_status\", max_interval = 5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQyg-v2t0UCe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Chi2(df, total_col, bad_col, overallRate):\n",
        "  df2 = df.copy()\n",
        "  df2[\"expected\"] = df[total_col].apply(lambda X: X * overallRate)\n",
        "  combined = zip(df2[\"expected\"], df2[bad_col])\n",
        "  chi = [(i[0] - i[1]) ** 2/i[0] for i in combined]\n",
        "  chi2 = sum(chi)\n",
        "  return chi2\n",
        "\n",
        "Chi2(df_tarin[\"loan_amnt\"], total_col, bad_col, overallRate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmaqTFdL89bo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#df_train.groupby([\"loan_amnt\"])[\"loan_status\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcOCDYJwytnZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = df_train[\"loan_amnt\"]\n",
        "colLevels = set(df) \n",
        "colLevels = sorted(list(colLevels))\n",
        "N_distinct = len(colLevels) \n",
        "\n",
        "total = df_train.groupby([\"loan_amnt\"])[\"loan_status\"].count()\n",
        "\n",
        "total = pd.DataFrame({\"total\": total})\n",
        "\n",
        "bad = df_train.groupby([\"loan_amnt\"])[\"loan_status\"].sum()\n",
        "regroup = total.merge(bad, left_index = True, right_index = True, how = \"left\")\n",
        "print(regroup)\n",
        "regroup.reset_index(level = 0, inplace = True)\n",
        "print(regroup)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmjJ6YsZzpB3",
        "colab_type": "text"
      },
      "source": [
        "### 5.3.1 对无缺失值的连续型数值变量分箱\n",
        "\n",
        "对连续型数值变量实施分箱，得到切割点\n",
        "8 intervals for the features\n",
        "\n",
        "loan_amnt, int_rate, annual_inc, dti, open_acc, revol_bal, revol_util, total_acc, cre_hist\n",
        "\n",
        " |loan_amnt| int_rate|annual_inc|dti|open_acc|"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SA8Dt0VRtPkJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_feas_to_box = [\"loan_amnt\", \"int_rate\", \"annual_inc\", \"dti\", \"open_acc\", \"revol_bal\", \"revol_util\", \"total_acc\", \"cre_hist\"]\n",
        "df_train[num_feas_to_box].head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usmzrbkL0AkW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# by the sequence of the num_feas_to_box\n",
        "cut_points_list = [\n",
        "    [4450.0, 6000.0, 7725.0, 9750.0, 12200.0, 14275.0, 17800.0, 22875.0, 29900.0],\n",
        "    [6.03, 7.75, 9.99, 13.99, 16.01, 17.49, 18.99, 20.3, 21.67],\n",
        "    [26004.0, 37225.0, 40560.0, 48156.0, 51669.0, 62851.0, 74143.68, 89092.0, 115275.0],\n",
        "    [2.53, 4.18, 6.45, 9.65, 12.47, 15.62, 19.31, 21.03, 23.87],\n",
        "    [2.0, 3.0, 5.0, 6.0, 15.0, 20.0, 26.0, 30.0, 32.0], \n",
        "    [1098.1, 2838.2, 4632.6, 6577.0, 8841.0, 11465.0, 14833.0, 19816.2, 29139.5], \n",
        "    [10.17, 21.4, 29.7, 42.0, 55.3, 68.0, 80.5, 92.7, 96.5], \n",
        "    [3.0, 4.0, 14.0, 26.0, 36.0, 43.0, 50.0, 60.0, 69.0],\n",
        "    [55, 104, 186, 220, 314, 376, 425, 473, 525]    \n",
        "]\n",
        "\n",
        "cut_points_dict = {}\n",
        "for i in range(len(num_feas_to_box)):\n",
        "  cut_points_dict[num_feas_to_box[i]] = cut_points_list[i]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvE7rr2P-_d2",
        "colab_type": "text"
      },
      "source": [
        "制作切割点字典：\n",
        "\n",
        "实施分箱："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-RtvdQX_CIF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 用卡方分箱得到的分割点经行分箱，要注意， 所有数据集中同样的特征区间都必须是相同的\n",
        "def box_col_to_df(df_to_box, col, cut_points):\n",
        "  bins = [-10.0] + cut_points + [10000000.0]\n",
        "  # 如果有重复切割点， duplicates = 'drop'去重\n",
        "  df_to_box[col] = pd.cut(df_to_box[col], bins = bins, include_lowest = True, duplicates = \"drop\",\n",
        "                         labels = range(len(bins) - 1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwHeAhGBupqe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for data in df_list:\n",
        "  box_col_to_df(data, num_feas_to_box, cut_points_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82J0DV6B_o3R",
        "colab_type": "text"
      },
      "source": [
        "### 5.3.2 对含有缺失值的连续型数值变量分箱\n",
        "\n",
        "对两个比较特殊的变量mths_since_last_delinq’和'mths_since_last_record'，它们也是属于连续型特征变量，但是却存在缺失值，分箱策略是将缺失值作为一类，其他类进行卡方分箱。\n",
        "\n",
        "得到２者的切割点：\n",
        "\n",
        "‘mths_since_last_delinq’:\n",
        "[19.0, 33.0, 38.0, 63.0]\n",
        "\n",
        "'mths_since_last_record':\n",
        "\n",
        "[46.0, 68.0, 79.0, 82.0]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7Bmn5x8_CTW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 对 mths_since_last_delinq & mths_since_last_record 分箱编写函数\n",
        "def box_mth_col(df_to_box, mth_col, bins):\n",
        "  bins = [0.0] + [1.0] + bins + [150.0]\n",
        "  df_to_box[mth_col][df_to_box[mth_col].notnull()] = pd.cut(\n",
        "  df_to_box[mth_col][df_to_box[mth_col].notnull()], bins = bins, include_lowest = True, labels = range(len(bins) - 1))\n",
        "  df_to_box[mth_col][df_to_box[mth_col].isnull()] = -1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTFTCsnmApGq",
        "colab_type": "text"
      },
      "source": [
        "将所有变量分箱后的效果如下：\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yA-7lBE6uVnH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train.head(3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlD_muH5Aq6l",
        "colab_type": "text"
      },
      "source": [
        "# 6. Model  building\n",
        "\n",
        "经过之前的数据处理和特征工程，得到了分箱后的，规整的数据，并且找到了对评分卡来说，高预测性能的特征。\n",
        "\n",
        "这部分我会建立１个传统的评分卡和１个较复杂解释性较差但预测性较好的随机森林模型。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hmyv-E-IAzw3",
        "colab_type": "text"
      },
      "source": [
        "### 6.1 建立评分卡\n",
        "\n",
        "在５．２．２节，得到了每个特征中不同属性的woe值，它的含义是该分类对“好结果”的贡献度。\n",
        "\n",
        "建立评分卡的步骤如下：\n",
        "\n",
        "用woe值替换相应位置的属性值\n",
        "\n",
        "建立逻辑回归模型\n",
        "\n",
        "根据回归结果计分\n",
        "\n",
        "实施："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUjV_uyqAnup",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 用WOE 替代相应位置的属性值\n",
        "\"\"\"\n",
        "df_card_train = df_train[valid_feas + [\"loan_status\"]]\n",
        "df_card_test = df_test[valid_feas + [\"loan_status\"]]\n",
        "\n",
        "for i in range(len(valid_feas)):\n",
        "  df_card_train[valid_feas[i]].replace(iv_dict[valid_feas[i]][\"WOE\"][\"WOE\"], inplace = True)\n",
        "  df_card_test[valid_feas[i]].replace(iv_dict[valid_feas[i]][\"WOE\"][\"WOE\"], inplace = True)\n",
        "\"\"\"  \n",
        "# 简历逻辑回归模型\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "LR = LogisticRegression()\n",
        "LR.fit(df_card_train.drop(\"loan_status\", axis = 1), df_card_train[\"loan_status\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHKcwHdSBuuC",
        "colab_type": "text"
      },
      "source": [
        "评估模型的预测性能"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XB_7ASHvBwVo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "auc = roc_auc_score(LR.predict(df_card_test.drop(\"loan_status\", axis = 1)), df_card_test[\"loan_status\"])\n",
        "fpr, tpr, thre = roc_curve(LR.predict(df_card_test.drop(\"loan_status\", axis = 1)), df_card_test[\"loan_status\"])\n",
        "ks = max(tpr-fpr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stmA9Zu8Bwnt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"auc:{}     ks: {}\".format(auc, ks))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwVJ_LorCcNL",
        "colab_type": "text"
      },
      "source": [
        "auc约为0.672,ks值约为0.35,在评分卡建模中，ks值大于0.3，说明这是一个基本可用的模型。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hE9XYSJgCeWl",
        "colab_type": "text"
      },
      "source": [
        "输出评分卡："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1nWTlOWBwgo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 输出评分卡\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# df_card_train = sm.add_constant(df_card_train)\n",
        "\n",
        "logit = sm.Logit(df_card_train[\"loan_status\"].values, df_card_train.drop(\"loan_status\", axis = 1).values).fit()\n",
        "\n",
        "B = 20/np.log(2)\n",
        "A = 600 + 20 * np.log(1/60)/np.log(2)\n",
        "besescore = round(A - B * logit.params[0] 0)\n",
        "scorecard = []\n",
        "\n",
        "# features, remove(\"loan_status\")\n",
        "for j, i in enumerate(valid_feas + [\"loan_amnt\"]):\n",
        "  woe = iv_dict[i][\"WOE\"][\"WOE\"]\n",
        "  interval = []\n",
        "  scores = []\n",
        "  for key, value in woe.items():\n",
        "    score = round(-(value * logit.params[j] * B))\n",
        "    scores.append(score)\n",
        "    interval.append(key)\n",
        "  data = pd.DataFrame({\"interval\": interval, \"scores\": scores})\n",
        "  scorecard.append(data)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gx7-axwDbfk",
        "colab_type": "text"
      },
      "source": [
        "### 6.2 建立随机森林模型\n",
        "\n",
        "评分卡的优势在于简单明了，但是它无法包含更多的信息。随机森林的是以决策树为弱学习模型通过bagging方法构造出的强学习模型，它能容纳更多的信息，同时通过多模型投票，又能很好的避免过拟合的影响，它正好弥补了这一缺陷。这一模型可以作为评分卡的参考。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vI8griTbfgSh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train.columns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3c8A2mVfMw_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 暂时去掉purpose,看是不是因为purpoes是object的问题, \n",
        "for data in df_list:\n",
        "  data.drop([\"purpose\",\"mths_since_last_delinq\", \"mths_since_last_record\", \"revol_util\"], axis = 1, inplace = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFhdhvyugpNV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Training dataset:\")\n",
        "print(df_train.isnull().sum())\n",
        "\n",
        "print(\"Validation dataset:\")\n",
        "print(df_val.isnull().sum())\n",
        "\n",
        "print(\"Testing dataset:\")\n",
        "print(df_test.isnull().sum())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "791cAwjBDg_3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Random Forest\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf = RandomForestClassifier()\n",
        "rf.fit(df_train.drop(\"loan_status\", axis = 1).values, df_train[\"loan_status\"].values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TzjQ1xEEDmu",
        "colab_type": "text"
      },
      "source": [
        "用网格搜索的方式优化逻辑回归森林模型：\n",
        "\n",
        "回归森林模型中，n_estimators表示底层决策树个数，一般来说，树的个数越多，模型的稳定性越强，但是它的增大要受限于计算性能"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpTtH2BzDhJ9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 网格搜索模型， 对随机森林中的DT个数经行遍历\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "rf = RandomForestClassifier()\n",
        "parameters = {\"n_estimators\": [5, 10, 20, 35, 50, 100]}\n",
        "\n",
        "gs = GridSearchCV(estimator = rf, param_grid = parameters, scoring = \"roc_auc\", cv = 5, n_jobs = -1)\n",
        "\n",
        "grid_result = gs.fit(df_train.drop(\"loan_status\", axis = 1).values, df_train.loan_status.values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQumZm_N-Yrk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "grid_result.best_score_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3G2F43oSHYwL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "grid_result.best_params_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIqgrEmzHgqm",
        "colab_type": "text"
      },
      "source": [
        "From the result above, the best roc_auc is 66.39% when n_estimators = 100."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJhQwVwDHd-v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZoIBWn4-ghr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_learning_curve(estimator, title, X, y, ylim = None, cv = None, \n",
        "                        n_jobs = 1, train_sizes = np.linspace(0.05, 1.0, 20), \n",
        "                        verbose = 0, plot = True):\n",
        "  train_sizes, train_scores, test_scores, = learning_curve(estimator, X, y, \n",
        "                                                           cv = cv, n_jobs = n_jobs, \n",
        "                                                           train_sizes = train_sizes, \n",
        "                                                           verbose = verbose)\n",
        "  train_scores_mean = np.mean(train_scores, axis = 1)\n",
        "  train_scores_std = np.std(train_scores, axis = 1)\n",
        "  test_scores_mean = np.mean(test_scores, axis = 1)\n",
        "  test_scores_std = np.std(test_scores, axis = 1)\n",
        "  \n",
        "  if plot:\n",
        "    plt.figure()\n",
        "    plt.title(title)\n",
        "    if ylim is not None:\n",
        "      plt.ylim(*ylim)\n",
        "      plt.xlabel(\"Training size\")\n",
        "      plt.ylabel(\"Score\")\n",
        "      plt.gca().invert_yaxis()\n",
        "      plt.grid()\n",
        "      plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
        "                      train_scores_mean + train_score_scores_std, alpha = 0.1, color = \"b\")\n",
        "      plt.fill_between(train_sizes, test_scores_mean - test_scores_std, \n",
        "                      test_scores_mean + test_scores_std, alpha = 0.1, color = \"r\")\n",
        "      plt.plot(train_sizes, train_scores_mean, \"o-\", color = \"b\", label = \"Training\")\n",
        "      plt.plot(train_sizes, test_scores_mean, \"o-\", color = \"r\", label = \"Validation\")\n",
        "      plt.legend(loc = \"best\")\n",
        "      plt.draw()\n",
        "      plt.show()\n",
        "  \n",
        "  midpoint = ((train_scores_mean[-1] + train_scores_std[-1]) + (test_scores_mean[-1] - test_scores_std[-1])) / 2\n",
        "  diff = (train_scores_mean[-1] + train_scores_std[-1] - test_scores_mean[-1] - test_scores_std[-1])\n",
        "  return midpoint, diff\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v49Ths_wEuGx",
        "colab_type": "text"
      },
      "source": [
        "# 7. conclusion\n",
        "在评估小微企业信贷风险时，个人信用评分只是其中一个环节，还应该综合考虑借贷人的经营情况,出借方的风险偏好等其他因素，构建风控策略和风控系统。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9inHVvelkndc",
        "colab_type": "text"
      },
      "source": [
        "## useful reference:\n",
        "URL_1 = 'https://blog.csdn.net/zs15321583801/article/details/89485951' **(example)**\n",
        "\n",
        "URL_2 = \"https://www.jianshu.com/p/a8037a38e219\"\n",
        "\n",
        "URL_3 = \"https://zhuanlan.zhihu.com/p/21550547\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ak4vPJKDBtxp",
        "colab_type": "text"
      },
      "source": [
        "DataCamp = \"https://www.datacamp.com/community/tutorials/machine-learning-python\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2UbK4O5ktOL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}